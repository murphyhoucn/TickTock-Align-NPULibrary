#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TickTock-Deep-Learning-Align Library

åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒå¯¹é½åº“ï¼Œä¸“é—¨é’ˆå¯¹å»ºç­‘ç‰©æ—¶é—´åºåˆ—å›¾åƒã€‚
ä½¿ç”¨SuperGlue/SuperPointæˆ–LoFTRç­‰ç°ä»£æ·±åº¦å­¦ä¹ ç‰¹å¾åŒ¹é…æ–¹æ³•ã€‚

åŠŸèƒ½ç‰¹ç‚¹:
- åŸºäºæ·±åº¦å­¦ä¹ çš„ç‰¹å¾æå–å’ŒåŒ¹é…
- å¯¹å…‰ç…§å˜åŒ–ã€å­£èŠ‚å˜åŒ–é²æ£’
- æ”¯æŒæ—¥å¤œå›¾åƒçš„ç»Ÿä¸€å¤„ç†
- å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹å¯é€‰
- è‡ªåŠ¨å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
"""

import cv2
import numpy as np
from pathlib import Path
import argparse
import logging
import time
import sys
import warnings
import requests
from tqdm import tqdm
warnings.filterwarnings('ignore')

# æ·±åº¦å­¦ä¹ å¯¼å…¥
import torch
import torch.nn.functional as F
TORCH_AVAILABLE = True

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # æŒ‡å®šä½¿ç”¨çš„GPU

# å¯é€‰çš„ç‰¹æ®Šåº“å¯¼å…¥
try:
    import kornia as K
    import kornia.feature as KF
    # å°è¯•å¯¼å…¥å¯ç”¨çš„ç‰¹å¾æ£€æµ‹å™¨
    KORNIA_AVAILABLE = True
    
    # æ£€æŸ¥å¯ç”¨çš„ç‰¹å¾æ£€æµ‹å™¨
    SIFT_AVAILABLE = hasattr(KF, 'SIFTFeature')
    LOFTR_AVAILABLE = hasattr(KF, 'LoFTR')
    
except ImportError:
    KORNIA_AVAILABLE = False
    SIFT_AVAILABLE = False
    LOFTR_AVAILABLE = False

try:
    from scipy.ndimage import maximum_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DeepLearningAlign:
    """
    åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒå¯¹é½ç±»
    
    æ”¯æŒå¤šç§æ·±åº¦å­¦ä¹ ç‰¹å¾åŒ¹é…æ–¹æ³•ï¼Œå¯¹ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å¤„ç†çš„åœºæ™¯æä¾›æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚
    """
    
    def __init__(self, input_dir="Lib", output_dir="DL-Align", reference_index=0, method="superpoint"):
        """
        åˆå§‹åŒ–æ·±åº¦å­¦ä¹ å¯¹é½å™¨
        
        Args:
            input_dir (str): è¾“å…¥å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            output_dir (str): è¾“å‡ºå¯¹é½å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            reference_index (int): å‚è€ƒå›¾åƒç´¢å¼•
            method (str): ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ æ–¹æ³• ('superpoint', 'loftr', 'sift_dl')
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.reference_index = reference_index
        self.method = method
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        if TORCH_AVAILABLE:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            if torch.cuda.is_available():
                logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
                logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        else:
            self.device = 'cpu'
            logger.warning("PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUå’Œä¼ ç»Ÿæ–¹æ³•")
        
        # åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹
        self.init_models()
        
    def init_models(self):
        """åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹"""
        logger.info(f"åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹: {self.method}")
        
        if self.method == "superpoint":
            self.init_superpoint()
        elif self.method == "loftr":
            self.init_loftr()
        elif self.method == "sift_dl":
            self.init_sift_dl()
        elif self.method == "lightweight":
            self.init_lightweight_features()
        else:
            logger.warning(f"æœªçŸ¥æ–¹æ³• {self.method}ï¼Œå›é€€åˆ°ä¼ ç»ŸSIFT")
            self.init_traditional_sift()
    
    def init_superpoint(self):
        """åˆå§‹åŒ–Korniaç‰¹å¾æ£€æµ‹å™¨ï¼ˆä¼˜å…ˆä½¿ç”¨LoFTRï¼Œå›é€€åˆ°SIFTï¼‰"""
        if KORNIA_AVAILABLE and TORCH_AVAILABLE:
            try:
                # é¦–å…ˆå°è¯•ä½¿ç”¨æœ¬åœ°LoFTRæ¨¡å‹
                local_loftr_path = "/mnt/houjinliang/MyDevProject/TickTock-NPUEveryday/Align/loftr_outdoor.ckpt"
                
                # æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰LoFTRæ¨¡å‹æ–‡ä»¶
                if LOFTR_AVAILABLE:
                    if not Path(local_loftr_path).exists():
                        logger.info("æœ¬åœ°LoFTRæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
                        success = self.download_loftr_model(local_loftr_path)
                        if not success:
                            logger.warning("LoFTRæ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°å…¶ä»–æ–¹æ³•")
                            self.init_fallback_method()
                            return
                    
                    # ä½¿ç”¨æœ¬åœ°LoFTRæ¨¡å‹
                    import torch
                    state_dict = torch.load(local_loftr_path, map_location=self.device)
                    self.loftr_matcher = KF.LoFTR(pretrained=None)
                    self.loftr_matcher.load_state_dict(state_dict['state_dict'])
                    self.loftr_matcher = self.loftr_matcher.to(self.device).eval()
                    logger.info("LoFTRæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
                    self.model_available = True
                    self.use_loftr = True
                    return
                else:
                    logger.warning("æœªæ‰¾åˆ°LoFTRæ”¯æŒï¼Œå›é€€åˆ°å…¶ä»–æ–¹æ³•")
                    self.init_fallback_method()
                    return
            except Exception as e:
                logger.warning(f"Korniaç‰¹å¾æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        logger.warning("Korniaä¸å¯ç”¨ï¼Œä½¿ç”¨è½»é‡çº§ç‰¹å¾æå–å™¨")
        self.init_lightweight_features()
    
    def download_loftr_model(self, local_path):
        """ä¸‹è½½LoFTRæ¨¡å‹æ–‡ä»¶"""
        try:
            # LoFTR outdooræ¨¡å‹ä¸‹è½½é“¾æ¥
            model_url = "https://github.com/zju3dv/LoFTR/releases/download/v1.0/outdoor_ds.ckpt"
            
            logger.info(f"æ­£åœ¨ä¸‹è½½LoFTRæ¨¡å‹åˆ°: {local_path}")
            
            # åˆ›å»ºç›®å½•
            Path(local_path).parent.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(model_url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(local_path, 'wb') as f, tqdm(
                desc="ä¸‹è½½LoFTRæ¨¡å‹",
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024,
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
            
            logger.info(f"LoFTRæ¨¡å‹ä¸‹è½½å®Œæˆ: {local_path}")
            return True
            
        except Exception as e:
            logger.error(f"LoFTRæ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            # æ¸…ç†å¯èƒ½çš„éƒ¨åˆ†ä¸‹è½½æ–‡ä»¶
            if Path(local_path).exists():
                Path(local_path).unlink()
            return False
    
    def init_fallback_method(self):
        """åˆå§‹åŒ–å›é€€æ–¹æ³•"""
        if SIFT_AVAILABLE:
            # å›é€€åˆ°Kornia SIFTç‰¹å¾
            self.kornia_sift = KF.SIFTFeature(num_features=2000).to(self.device).eval()
            logger.info("Kornia SIFTæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            self.model_available = True
            self.use_loftr = False
        else:
            logger.warning("å›é€€åˆ°è½»é‡çº§ç‰¹å¾æå–å™¨")
            self.init_lightweight_features()
    
    def init_loftr(self):
        """åˆå§‹åŒ–LoFTRæ¨¡å‹"""
        try:
            # å°è¯•ä½¿ç”¨ç¬¬ä¸‰æ–¹LoFTRå®ç°
            logger.info("LoFTRæ¨¡å‹éœ€è¦é¢å¤–å®‰è£…ï¼Œå›é€€åˆ°è½»é‡çº§ç‰¹å¾æå–")
            self.init_lightweight_features()
        except Exception as e:
            logger.warning(f"LoFTRåˆå§‹åŒ–å¤±è´¥: {e}")
            self.init_lightweight_features()
    
    def init_sift_dl(self):
        """åˆå§‹åŒ–SIFT+æ·±åº¦å­¦ä¹ åŒ¹é…çš„æ··åˆæ–¹æ³•"""
        # ä½¿ç”¨ä¼ ç»ŸSIFTæå–ç‰¹å¾ï¼Œæ·±åº¦å­¦ä¹ è¿›è¡ŒåŒ¹é…
        self.sift = cv2.SIFT_create(nfeatures=2000)
        self.init_lightweight_matcher()
        logger.info("SIFT+DLæ··åˆæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        self.model_available = True
    
    def init_lightweight_features(self):
        """åˆå§‹åŒ–è½»é‡çº§æ·±åº¦å­¦ä¹ ç‰¹å¾æå–å™¨"""
        if not TORCH_AVAILABLE:
            self.init_traditional_sift()
            return
            
        # æ›´å¼ºå¤§çš„CNNç‰¹å¾æå–å™¨
        class EnhancedFeatureExtractor(torch.nn.Module):
            def __init__(self):
                super().__init__()
                # æ”¹è¿›çš„CNNæ¶æ„
                self.conv1 = torch.nn.Conv2d(1, 64, 3, padding=1)
                self.bn1 = torch.nn.BatchNorm2d(64)
                self.conv2 = torch.nn.Conv2d(64, 128, 3, padding=1)
                self.bn2 = torch.nn.BatchNorm2d(128)
                self.conv3 = torch.nn.Conv2d(128, 256, 3, padding=1)
                self.bn3 = torch.nn.BatchNorm2d(256)
                self.conv4 = torch.nn.Conv2d(256, 512, 3, padding=1)
                self.bn4 = torch.nn.BatchNorm2d(512)
                
                # ç‰¹å¾æè¿°å­å¤´
                self.descriptor_head = torch.nn.Conv2d(512, 256, 1)
                
                # å…³é”®ç‚¹æ£€æµ‹å¤´
                self.keypoint_head = torch.nn.Conv2d(512, 1, 1)
                
            def forward(self, x):
                # ç‰¹å¾æå–
                x = F.relu(self.bn1(self.conv1(x)))
                x = F.max_pool2d(x, 2)  # 1/2
                
                x = F.relu(self.bn2(self.conv2(x)))
                x = F.max_pool2d(x, 2)  # 1/4
                
                x = F.relu(self.bn3(self.conv3(x)))
                x = F.max_pool2d(x, 2)  # 1/8
                
                features = F.relu(self.bn4(self.conv4(x)))
                
                # ç”Ÿæˆæè¿°å­å’Œå…³é”®ç‚¹
                descriptors = F.normalize(self.descriptor_head(features), p=2, dim=1)
                keypoint_scores = torch.sigmoid(self.keypoint_head(features))
                
                return {
                    'descriptors': descriptors,
                    'keypoint_scores': keypoint_scores,
                    'features': features
                }
        
        self.feature_extractor = EnhancedFeatureExtractor().to(self.device).eval()
        logger.info("å¢å¼ºç‰ˆç‰¹å¾æå–å™¨åˆå§‹åŒ–æˆåŠŸ")
        self.model_available = True
    
    def init_lightweight_matcher(self):
        """åˆå§‹åŒ–è½»é‡çº§æ·±åº¦å­¦ä¹ åŒ¹é…å™¨"""
        class LightweightMatcher(torch.nn.Module):
            def __init__(self, feature_dim=128):
                super().__init__()
                self.matcher = torch.nn.Sequential(
                    torch.nn.Linear(feature_dim * 2, 256),
                    torch.nn.ReLU(),
                    torch.nn.Linear(256, 128),
                    torch.nn.ReLU(),
                    torch.nn.Linear(128, 1),
                    torch.nn.Sigmoid()
                )
            
            def forward(self, desc1, desc2):
                # è®¡ç®—æè¿°ç¬¦ç›¸ä¼¼åº¦
                combined = torch.cat([desc1, desc2], dim=-1)
                similarity = self.matcher(combined)
                return similarity
        
        self.matcher = LightweightMatcher().to(self.device).eval()
        self.model_available = True
    
    def init_traditional_sift(self):
        """åˆå§‹åŒ–ä¼ ç»ŸSIFTä½œä¸ºå›é€€æ–¹æ¡ˆ"""
        self.sift = cv2.SIFT_create(nfeatures=2000)

        logger.info("ä¼ ç»ŸSIFTåˆå§‹åŒ–æˆåŠŸï¼ˆå›é€€æ–¹æ¡ˆï¼‰")
        self.model_available = True
    
    def get_image_files(self):
        """è·å–è¾“å…¥ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒæ–‡ä»¶"""
        image_files = []
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        
        for ext in image_extensions:
            image_files.extend(list(self.input_dir.rglob(f"*{ext}")))
            image_files.extend(list(self.input_dir.rglob(f"*{ext.upper()}")))
        
        image_files = list(set([str(f) for f in image_files]))
        image_files.sort()
        return image_files
    
    def preprocess_image(self, img, target_size=512):
        """
        é¢„å¤„ç†å›¾åƒç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹
        
        Args:
            img: è¾“å…¥å›¾åƒ
            target_size: ç›®æ ‡å°ºå¯¸
            
        Returns:
            processed_img: é¢„å¤„ç†åçš„å›¾åƒtensor
            scale_factor: ç¼©æ”¾å› å­
        """
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img
        
        # è®¡ç®—ç¼©æ”¾å› å­
        h, w = gray.shape
        scale = target_size / max(h, w)
        new_h, new_w = int(h * scale), int(w * scale)
        
        # è°ƒæ•´å¤§å°
        resized = cv2.resize(gray, (new_w, new_h))
        
        # è½¬æ¢ä¸ºtensor
        tensor_img = torch.from_numpy(resized).float() / 255.0
        tensor_img = tensor_img.unsqueeze(0).unsqueeze(0).to(self.device)
        
        return tensor_img, scale, (new_h, new_w)
    
    def extract_features_kornia(self, img):
        """ä½¿ç”¨Korniaç‰¹å¾æ£€æµ‹å™¨æå–ç‰¹å¾"""
        try:
            if hasattr(self, 'use_loftr') and self.use_loftr:
                # å¯¹äºLoFTRï¼Œè¿”å›é¢„å¤„ç†åçš„tensor
                tensor_result, scale, bbox = self.preprocess_for_loftr(img)
                logger.info("LoFTRç‰¹å¾å‡†å¤‡å®Œæˆ")
                return None, tensor_result  # è¿”å›Noneå’Œtensorä¾›åç»­LoFTRåŒ¹é…ä½¿ç”¨
                    
            elif hasattr(self, 'kornia_sift'):
                # é¢„å¤„ç†å›¾åƒ
                if len(img.shape) == 3:
                    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                else:
                    gray = img
                    
                # è½¬æ¢ä¸ºtensor
                img_tensor = torch.from_numpy(gray).float().unsqueeze(0).unsqueeze(0).to(self.device) / 255.0
                
                # ä½¿ç”¨Kornia SIFT
                lafs, responses, descriptors = self.kornia_sift(img_tensor)
                
                if lafs.shape[1] > 0:
                    # è½¬æ¢LAFsåˆ°å…³é”®ç‚¹
                    lafs_np = lafs[0].cpu().numpy()  # [N, 2, 3]
                    keypoints = []
                    
                    for laf in lafs_np:
                        # LAFæ ¼å¼è½¬æ¢ä¸ºå…³é”®ç‚¹åæ ‡
                        x, y = laf[0, 2], laf[1, 2]  # ä¸­å¿ƒåæ ‡
                        # è®¡ç®—å°ºåº¦
                        scale = np.sqrt(laf[0, 0]**2 + laf[0, 1]**2)
                        keypoints.append(cv2.KeyPoint(x=float(x), y=float(y), size=float(scale)))
                    
                    desc_np = descriptors[0].cpu().numpy().T  # [N, 128]
                    
                    logger.info(f"Kornia SIFTç‰¹å¾æå–: {len(keypoints)}ä¸ªå…³é”®ç‚¹")
                    return keypoints, desc_np
                else:
                    logger.warning("Kornia SIFTæœªæ£€æµ‹åˆ°ç‰¹å¾ç‚¹")
                    return self.extract_features_sift(img)
                        
        except Exception as e:
            logger.error(f"Korniaç‰¹å¾æå–å¤±è´¥: {e}")
            return self.extract_features_sift(img)
    
    def extract_features_lightweight(self, img):
        """ä½¿ç”¨å¢å¼ºç‰ˆç½‘ç»œæå–ç‰¹å¾"""
        try:
            processed_img, scale, (h, w) = self.preprocess_image(img, target_size=640)
            
            with torch.no_grad():
                # ç‰¹å¾æå–
                output = self.feature_extractor(processed_img)
                descriptors_map = output['descriptors']
                keypoint_scores = output['keypoint_scores']
                
                # å…³é”®ç‚¹æ£€æµ‹ï¼šä½¿ç”¨éæœ€å¤§å€¼æŠ‘åˆ¶
                scores = keypoint_scores[0, 0].cpu().numpy()
                
                # ä½¿ç”¨æ›´å¥½çš„å…³é”®ç‚¹æ£€æµ‹
                if SCIPY_AVAILABLE:
                    from scipy.ndimage import maximum_filter
                    local_max = maximum_filter(scores, size=3) == scores  # å‡å°çª—å£
                    coords = np.where(local_max & (scores > 0.15))  # é™ä½é˜ˆå€¼
                else:
                    # ç®€å•çš„å±€éƒ¨æœ€å¤§å€¼æ£€æµ‹
                    threshold = np.percentile(scores, 85)  # é™ä½ç™¾åˆ†ä½
                    coords = np.where(scores > max(threshold, 0.15))
                
                if len(coords[0]) == 0:
                    logger.warning("æœªæ£€æµ‹åˆ°è¶³å¤Ÿçš„å…³é”®ç‚¹")
                    return self.extract_features_sift(img)
                
                # é™åˆ¶å…³é”®ç‚¹æ•°é‡
                max_keypoints = 2000  # å¢åŠ å…³é”®ç‚¹æ•°é‡
                if len(coords[0]) > max_keypoints:
                    # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰æ‹©æœ€å¥½çš„
                    point_scores = scores[coords]
                    top_indices = np.argsort(point_scores)[-max_keypoints:]
                    coords = (coords[0][top_indices], coords[1][top_indices])
                
                # æ„å»ºå…³é”®ç‚¹å’Œæè¿°ç¬¦
                keypoints = []
                descriptors = []
                
                desc_map = descriptors_map[0].cpu().numpy()  # [256, H, W]
                
                for y, x in zip(coords[0], coords[1]):
                    # ç¼©æ”¾å›åŸå§‹å›¾åƒåæ ‡
                    scale_factor = 8  # ä¸‹é‡‡æ ·å€æ•°
                    orig_x = (x * scale_factor) / scale
                    orig_y = (y * scale_factor) / scale
                    
                    if 0 <= orig_x < img.shape[1] and 0 <= orig_y < img.shape[0]:
                        keypoints.append(cv2.KeyPoint(x=float(orig_x), y=float(orig_y), size=8.0))
                        
                        # æå–æè¿°ç¬¦
                        if y < desc_map.shape[1] and x < desc_map.shape[2]:
                            desc = desc_map[:, y, x]  # [256]
                            descriptors.append(desc)
                
                if descriptors and len(descriptors) > 10:  # æé«˜æœ€å°ç‰¹å¾ç‚¹è¦æ±‚
                    descriptors = np.array(descriptors)  # [N, 256]
                    logger.info(f"æ·±åº¦å­¦ä¹ ç‰¹å¾æå–: {len(keypoints)}ä¸ªå…³é”®ç‚¹")
                    return keypoints, descriptors
                else:
                    logger.warning(f"æ·±åº¦å­¦ä¹ ç‰¹å¾æå–ç»“æœä¸ä½³({len(descriptors) if descriptors else 0}ä¸ªç‰¹å¾ç‚¹)ï¼Œåˆ‡æ¢åˆ°SIFT")
                    return self.extract_features_sift(img)
                    
        except Exception as e:
            logger.error(f"æ·±åº¦å­¦ä¹ ç‰¹å¾æå–å¤±è´¥: {e}")
            return self.extract_features_sift(img)
    
    def extract_features_sift(self, img):
        """ä½¿ç”¨ä¼ ç»ŸSIFTæå–ç‰¹å¾"""
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img
            
        keypoints, descriptors = self.sift.detectAndCompute(gray, None)
        return keypoints, descriptors
    
    def extract_features(self, img):
        """æ ¹æ®æ–¹æ³•æå–ç‰¹å¾"""
        if self.method == "superpoint" and (hasattr(self, 'loftr_matcher') or hasattr(self, 'kornia_sift')):
            return self.extract_features_kornia(img)
        elif self.method in ["loftr", "lightweight"] and hasattr(self, 'feature_extractor'):
            # å°è¯•æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¦‚æœå¤±è´¥åˆ™è‡ªåŠ¨å›é€€åˆ°SIFT
            dl_result = self.extract_features_lightweight(img)
            if dl_result[1] is not None and len(dl_result[1]) >= 50:  # å¦‚æœæœ‰è¶³å¤Ÿçš„ç‰¹å¾ç‚¹
                return dl_result
            else:
                logger.info("æ·±åº¦å­¦ä¹ ç‰¹å¾ä¸è¶³ï¼Œä½¿ç”¨SIFTè¡¥å……")
                return self.extract_features_sift(img)
        else:
            return self.extract_features_sift(img)
    
    def preprocess_for_loftr(self, img, target_size=640):
        """ä¸“ä¸ºLoFTRä¼˜åŒ–çš„å›¾åƒé¢„å¤„ç†"""
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img
        
        # è·å–åŸå§‹å°ºå¯¸
        h, w = gray.shape
        
        # è®¡ç®—ç¼©æ”¾å› å­ï¼Œä¿æŒçºµæ¨ªæ¯”
        scale = min(target_size / w, target_size / h)
        new_w, new_h = int(w * scale), int(h * scale)
        
        # è°ƒæ•´å¤§å°
        resized = cv2.resize(gray, (new_w, new_h))
        
        # åˆ›å»ºå¡«å……çš„æ­£æ–¹å½¢å›¾åƒ
        padded = np.zeros((target_size, target_size), dtype=np.uint8)
        start_x = (target_size - new_w) // 2
        start_y = (target_size - new_h) // 2
        padded[start_y:start_y+new_h, start_x:start_x+new_w] = resized
        
        # è½¬æ¢ä¸ºtensor
        tensor_img = torch.from_numpy(padded).float().unsqueeze(0).unsqueeze(0).to(self.device) / 255.0
        
        return tensor_img, scale, (start_x, start_y, new_w, new_h)

    def match_features_loftr(self, ref_tensor_info, curr_img):
        """ä½¿ç”¨LoFTRè¿›è¡Œç‰¹å¾åŒ¹é… - ä¼˜åŒ–ç‰ˆ"""
        try:
            # é¢„å¤„ç†å½“å‰å›¾åƒ
            curr_tensor, curr_scale, (curr_sx, curr_sy, curr_w, curr_h) = self.preprocess_for_loftr(curr_img)
            
            # å¼•ç”¨å›¾åƒä¿¡æ¯ (ref_tensor_info å°±æ˜¯é¢„å¤„ç†åçš„tensor)
            ref_tensor = ref_tensor_info
            
            with torch.no_grad():
                # å‡†å¤‡è¾“å…¥æ•°æ®
                input_dict = {
                    'image0': ref_tensor,    # [1, 1, H, W]
                    'image1': curr_tensor    # [1, 1, H, W]
                }
                
                # è¿è¡ŒLoFTR
                correspondences = self.loftr_matcher(input_dict)
                
                # æå–åŒ¹é…ç»“æœ
                mkpts0 = correspondences['keypoints0'].cpu().numpy()  # [N, 2]
                mkpts1 = correspondences['keypoints1'].cpu().numpy()  # [N, 2]
                mconf = correspondences['confidence'].cpu().numpy()   # [N]
                
                # ä½¿ç”¨æ›´ä½çš„ç½®ä¿¡åº¦é˜ˆå€¼
                confidence_thresh = 0.1
                mask = mconf > confidence_thresh
                mkpts0_filtered = mkpts0[mask]
                mkpts1_filtered = mkpts1[mask]
                mconf_filtered = mconf[mask]
                
                if len(mkpts0_filtered) == 0:
                    logger.warning("âš ï¸  æ²¡æœ‰è¶³å¤Ÿç½®ä¿¡åº¦çš„åŒ¹é…ç‚¹")
                    return [], [], []
                
                # å°†åæ ‡ä»å¡«å……å›¾åƒè½¬æ¢å›åŸå§‹å›¾åƒåæ ‡
                # å‚è€ƒå›¾åƒåæ ‡è½¬æ¢ (å‡è®¾ä½¿ç”¨ç›¸åŒçš„é¢„å¤„ç†)
                ref_scale = curr_scale  # å‡è®¾å‚è€ƒå›¾åƒç”¨ç›¸åŒé¢„å¤„ç†
                mkpts0_orig = mkpts0_filtered.copy()
                mkpts0_orig[:, 0] = (mkpts0_orig[:, 0] - curr_sx) / ref_scale
                mkpts0_orig[:, 1] = (mkpts0_orig[:, 1] - curr_sy) / ref_scale
                
                # å½“å‰å›¾åƒåæ ‡è½¬æ¢
                mkpts1_orig = mkpts1_filtered.copy()
                mkpts1_orig[:, 0] = (mkpts1_orig[:, 0] - curr_sx) / curr_scale
                mkpts1_orig[:, 1] = (mkpts1_orig[:, 1] - curr_sy) / curr_scale
                
                # è¿‡æ»¤è¶…å‡ºåŸå§‹å›¾åƒè¾¹ç•Œçš„ç‚¹
                ref_h, ref_w = self.reference_shape[:2]
                curr_h, curr_w = curr_img.shape[:2]
                
                valid_mask = ((mkpts0_orig[:, 0] >= 0) & (mkpts0_orig[:, 0] < ref_w) &
                            (mkpts0_orig[:, 1] >= 0) & (mkpts0_orig[:, 1] < ref_h) &
                            (mkpts1_orig[:, 0] >= 0) & (mkpts1_orig[:, 0] < curr_w) &
                            (mkpts1_orig[:, 1] >= 0) & (mkpts1_orig[:, 1] < curr_h))
                
                mkpts0_final = mkpts0_orig[valid_mask]
                mkpts1_final = mkpts1_orig[valid_mask]
                mconf_final = mconf_filtered[valid_mask]
                
                # åˆ›å»ºOpenCVåŒ¹é…æ ¼å¼
                matches = []
                kp1_list = []
                kp2_list = []
                
                for i in range(len(mkpts0_final)):
                    kp1_list.append(cv2.KeyPoint(x=mkpts0_final[i, 0], y=mkpts0_final[i, 1], size=1))
                    kp2_list.append(cv2.KeyPoint(x=mkpts1_final[i, 0], y=mkpts1_final[i, 1], size=1))
                    matches.append(cv2.DMatch(i, i, float(1.0 - mconf_final[i])))
                
                logger.info(f"LoFTRæ‰¾åˆ° {len(matches)} ä¸ªæœ‰æ•ˆåŒ¹é…")
                return matches, kp1_list, kp2_list
                
        except Exception as e:
            logger.error(f"LoFTRåŒ¹é…å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [], [], []
    
    def match_features_dl(self, desc1, desc2, kp1, kp2):
        """ä½¿ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•åŒ¹é…ç‰¹å¾"""
        if desc1 is None or desc2 is None:
            return []
        
        try:
            # å¦‚æœæœ‰æ·±åº¦å­¦ä¹ åŒ¹é…å™¨
            if hasattr(self, 'matcher') and self.method == "sift_dl":
                return self.match_with_dl_matcher(desc1, desc2)
            else:
                return self.match_features_traditional(desc1, desc2)
                
        except Exception as e:
            logger.warning(f"æ·±åº¦å­¦ä¹ åŒ¹é…å¤±è´¥: {e}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            return self.match_features_traditional(desc1, desc2)
    
    def match_with_dl_matcher(self, desc1, desc2):
        """ä½¿ç”¨æ·±åº¦å­¦ä¹ åŒ¹é…å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        if hasattr(self, 'matcher'):
            # ä½¿ç”¨å¯å­¦ä¹ çš„åŒ¹é…å™¨
            return self.match_with_learned_matcher(desc1, desc2)
        else:
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦åŒ¹é…
            return self.match_with_cosine_similarity(desc1, desc2)
    
    def match_with_cosine_similarity(self, desc1, desc2):
        """ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œç‰¹å¾åŒ¹é…"""
        desc1_tensor = torch.from_numpy(desc1).float().to(self.device)
        desc2_tensor = torch.from_numpy(desc2).float().to(self.device)
        
        # å½’ä¸€åŒ–æè¿°ç¬¦
        desc1_norm = F.normalize(desc1_tensor, p=2, dim=1)
        desc2_norm = F.normalize(desc2_tensor, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(desc1_norm, desc2_norm.t())
        
        matches = []
        
        # äº’ç›¸æœ€è¿‘é‚»åŒ¹é…
        for i in range(similarity_matrix.shape[0]):
            # æ‰¾åˆ°æœ€ä¼¼å’Œæ¬¡ä¼¼çš„åŒ¹é…
            similarities = similarity_matrix[i]
            sorted_indices = torch.argsort(similarities, descending=True)
            
            if len(sorted_indices) >= 2:
                best_idx = sorted_indices[0]
                second_best_idx = sorted_indices[1]
                
                best_sim = similarities[best_idx]
                second_sim = similarities[second_best_idx]
                
                # Lowe's ratio test for cosine similarity
                ratio = best_sim / (second_sim + 1e-8)
                
                if best_sim > 0.5 and ratio > 1.1:  # é™ä½åŒ¹é…é˜ˆå€¼
                    # æ£€æŸ¥äº’ç›¸æœ€è¿‘é‚»
                    reverse_best = torch.argmax(similarity_matrix[:, best_idx])
                    if reverse_best == i:
                        distance = float(1.0 - best_sim)
                        matches.append(cv2.DMatch(i, int(best_idx), distance))
        
        return matches
    
    def match_with_learned_matcher(self, desc1, desc2):
        """ä½¿ç”¨å¯å­¦ä¹ çš„åŒ¹é…å™¨"""
        desc1_tensor = torch.from_numpy(desc1).float().to(self.device)
        desc2_tensor = torch.from_numpy(desc2).float().to(self.device)
        
        matches = []
        
        # è®¡ç®—æ‰€æœ‰å¯èƒ½çš„åŒ¹é…å¯¹
        with torch.no_grad():
            for i, d1 in enumerate(desc1_tensor):
                best_match_idx = -1
                best_similarity = 0
                
                for j, d2 in enumerate(desc2_tensor):
                    similarity = self.matcher(d1.unsqueeze(0), d2.unsqueeze(0))
                    
                    if similarity > best_similarity and similarity > 0.6:  # é˜ˆå€¼
                        best_similarity = similarity
                        best_match_idx = j
                
                if best_match_idx >= 0:
                    matches.append(cv2.DMatch(i, best_match_idx, float(1.0 - best_similarity)))
        
        return matches
    
    def match_features_traditional(self, desc1, desc2):
        """ä¼ ç»Ÿç‰¹å¾åŒ¹é…"""
        # ä½¿ç”¨FLANNåŒ¹é…å™¨
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        search_params = dict(checks=50)
        
        flann = cv2.FlannBasedMatcher(index_params, search_params)
        
        try:
            matches = flann.knnMatch(desc1, desc2, k=2)
            
            # Lowe's ratio test
            good_matches = []
            for match_pair in matches:
                if len(match_pair) == 2:
                    m, n = match_pair
                    if m.distance < 0.7 * n.distance:
                        good_matches.append(m)
            
            return good_matches
            
        except Exception as e:
            logger.warning(f"FLANNåŒ¹é…å¤±è´¥: {e}")
            return []
    
    def estimate_homography_robust(self, kp1, kp2, matches, ransac_thresh=5.0):
        """é²æ£’çš„å•åº”æ€§çŸ©é˜µä¼°è®¡"""
        if len(matches) < 4:  # OpenCVæœ€ä½è¦æ±‚æ˜¯4ä¸ªç‚¹
            logger.warning(f"åŒ¹é…ç‚¹æ•°é‡ä¸è¶³ ({len(matches)})ï¼Œæ— æ³•è®¡ç®—å•åº”æ€§çŸ©é˜µ")
            return None, 0
        
        # æå–åŒ¹é…ç‚¹åæ ‡
        src_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
        
        # ä½¿ç”¨RANSACä¼°è®¡å•åº”æ€§çŸ©é˜µï¼Œä¼˜åŒ–å‚æ•°
        homography, mask = cv2.findHomography(
            src_pts, dst_pts, 
            cv2.RANSAC, 
            ransacReprojThreshold=ransac_thresh,  # æ›´å®½æ¾çš„é˜ˆå€¼
            maxIters=10000,  # æ›´å¤šè¿­ä»£
            confidence=0.995  # æ›´é«˜ç½®ä¿¡åº¦
        )
        
        inliers = np.sum(mask) if mask is not None else 0
        
        # æ£€æŸ¥å•åº”æ€§çŸ©é˜µè´¨é‡
        if homography is not None:
            # æ£€æŸ¥æ¡ä»¶æ•°
            cond_num = np.linalg.cond(homography)
            if cond_num > 100000:  # æ¡ä»¶æ•°è¿‡é«˜
                logger.warning(f"å•åº”æ€§çŸ©é˜µæ¡ä»¶æ•°è¿‡é«˜: {cond_num:.0f}ï¼Œå°è¯•é™ä½ç²¾åº¦è¦æ±‚")
                # å°è¯•æ›´å®½æ¾çš„å‚æ•°
                homography, mask = cv2.findHomography(
                    src_pts, dst_pts, 
                    cv2.RANSAC, 
                    ransacReprojThreshold=ransac_thresh * 2,  # æ›´å®½æ¾
                    maxIters=5000,
                    confidence=0.99
                )
                inliers = np.sum(mask) if mask is not None else 0
        
        return homography, inliers
    
    def align_image(self, img, homography, reference_shape):
        """å¯¹é½å›¾åƒ"""
        if homography is None:
            logger.warning("å•åº”æ€§çŸ©é˜µä¸ºç©ºï¼Œè¿”å›è°ƒæ•´å¤§å°åçš„åŸå›¾åƒ")
            return cv2.resize(img, (reference_shape[1], reference_shape[0]))
        
        aligned_img = cv2.warpPerspective(
            img, homography, 
            (reference_shape[1], reference_shape[0]),
            flags=cv2.INTER_LINEAR,
            borderMode=cv2.BORDER_CONSTANT,
            borderValue=(0, 0, 0)
        )
        
        return aligned_img
    
    def process_images(self):
        """å¤„ç†æ‰€æœ‰å›¾åƒè¿›è¡Œå¯¹é½"""
        image_files = self.get_image_files()
        
        if not image_files:
            logger.error(f"åœ¨ {self.input_dir} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            return
        
        logger.info(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
        
        # è¯»å–å‚è€ƒå›¾åƒ
        if self.reference_index >= len(image_files):
            logger.error(f"å‚è€ƒå›¾åƒç´¢å¼• {self.reference_index} è¶…å‡ºèŒƒå›´")
            return
        
        reference_path = image_files[self.reference_index]
        reference_img = cv2.imread(reference_path)
        
        if reference_img is None:
            logger.error(f"æ— æ³•è¯»å–å‚è€ƒå›¾åƒ: {reference_path}")
            return
        
        logger.info(f"ä½¿ç”¨å‚è€ƒå›¾åƒ: {Path(reference_path).name}")
        
        # ä¿å­˜å‚è€ƒå›¾åƒå°ºå¯¸ä¾›LoFTRä½¿ç”¨
        self.reference_shape = reference_img.shape
        
        # æå–å‚è€ƒå›¾åƒç‰¹å¾
        ref_kp, ref_desc = self.extract_features(reference_img)
        
        # ç‰¹æ®Šå¤„ç†LoFTRæƒ…å†µ
        if hasattr(self, 'use_loftr') and self.use_loftr:
            if ref_desc is None:
                logger.error("å‚è€ƒå›¾åƒtensoræå–å¤±è´¥")
                return
            logger.info("å‚è€ƒå›¾åƒå·²å‡†å¤‡ç”¨äºLoFTRåŒ¹é…")
        else:
            if ref_desc is None:
                logger.error("å‚è€ƒå›¾åƒç‰¹å¾æå–å¤±è´¥")
                return
            logger.info(f"å‚è€ƒå›¾åƒæå–åˆ° {len(ref_kp)} ä¸ªç‰¹å¾ç‚¹")
        
        # ä¿å­˜å‚è€ƒå›¾åƒ
        ref_output_path = self.output_dir / Path(reference_path).name
        cv2.imwrite(str(ref_output_path), reference_img)
        logger.info(f"ä¿å­˜å‚è€ƒå›¾åƒ: {ref_output_path}")
        
        # å¤„ç†ç»Ÿè®¡
        success_count = 0
        total_processed = 0
        processing_report = []
        
        # å¤„ç†å…¶ä»–å›¾åƒ
        for i, img_path in enumerate(image_files):
            if i == self.reference_index:
                continue
            
            logger.info(f"å¤„ç†å›¾åƒ {i+1}/{len(image_files)}: {Path(img_path).name}")
            start_time = time.time()
            
            # è¯»å–å½“å‰å›¾åƒ
            current_img = cv2.imread(img_path)
            if current_img is None:
                logger.warning(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")
                continue
            
            total_processed += 1
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨LoFTR
            if hasattr(self, 'use_loftr') and self.use_loftr and hasattr(self, 'loftr_matcher'):
                # LoFTRç›´æ¥åŒ¹é…ä¸¤å¼ å›¾åƒ
                matches, matched_kp1, matched_kp2 = self.match_features_loftr(ref_desc, current_img)
                match_points = len(matches)
                
                logger.info(f"LoFTRæ‰¾åˆ° {match_points} ä¸ªåŒ¹é…ç‚¹")
                
                if match_points >= 4:
                    # ä½¿ç”¨æ›´å®½æ¾çš„RANSACå‚æ•°
                    homography, inliers = self.estimate_homography_robust(matched_kp1, matched_kp2, matches, ransac_thresh=8.0)
                    
                    if homography is not None:
                        logger.info(f"LoFTRå¯¹é½æˆåŠŸï¼Œå†…ç‚¹æ•°é‡: {inliers}")
                    else:
                        logger.warning("LoFTRå¯¹é½å¤±è´¥")
                else:
                    homography = None
                    inliers = 0
                    logger.warning("LoFTRåŒ¹é…ç‚¹ä¸è¶³")
            else:
                # ä¼ ç»Ÿçš„ç‰¹å¾æå–å’ŒåŒ¹é…
                curr_kp, curr_desc = self.extract_features(current_img)
                
                homography = None
                match_points = 0
                inliers = 0
                
                if curr_desc is not None:
                    # åŒ¹é…ç‰¹å¾ç‚¹
                    matches = self.match_features_dl(ref_desc, curr_desc, ref_kp, curr_kp)
                    match_points = len(matches)
                    
                    logger.info(f"æ‰¾åˆ° {match_points} ä¸ªåŒ¹é…ç‚¹")
                    
                    # ä¼°è®¡å•åº”æ€§çŸ©é˜µ
                    homography, inliers = self.estimate_homography_robust(ref_kp, curr_kp, matches)
                    
                    if homography is not None:
                        logger.info(f"æ·±åº¦å­¦ä¹ å¯¹é½æˆåŠŸï¼Œå†…ç‚¹æ•°é‡: {inliers}")
                    else:
                        logger.warning("æ·±åº¦å­¦ä¹ å¯¹é½å¤±è´¥")
            
            # å¯¹é½å›¾åƒ
            aligned_img = self.align_image(current_img, homography, reference_img.shape)
            
            # ä¿å­˜å¯¹é½åçš„å›¾åƒ
            output_path = self.output_dir / Path(img_path).name
            cv2.imwrite(str(output_path), aligned_img)
            
            processing_time = time.time() - start_time
            success = homography is not None
            
            if success:
                success_count += 1
            
            # è®°å½•å¤„ç†æŠ¥å‘Š
            report_entry = {
                'filename': Path(img_path).name,
                'method': self.method,
                'match_points': match_points,
                'inliers': inliers,
                'processing_time': processing_time,
                'success': success
            }
            processing_report.append(report_entry)
            
            logger.info(f"ä¿å­˜å¯¹é½å›¾åƒ: {output_path} (æ·±åº¦å­¦ä¹ , {processing_time:.2f}ç§’)")
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        logger.info("=" * 60)
        logger.info("æ·±åº¦å­¦ä¹ å¯¹é½å¤„ç†ç»Ÿè®¡:")
        logger.info(f"æ€»å›¾åƒæ•°é‡: {total_processed}")
        logger.info(f"æˆåŠŸå¯¹é½: {success_count}")
        logger.info(f"æˆåŠŸç‡: {success_count/total_processed*100:.1f}%")
        logger.info(f"ä½¿ç”¨æ–¹æ³•: {self.method}")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        self.generate_report(processing_report, success_count, total_processed)
        
        logger.info("æ·±åº¦å­¦ä¹ å›¾åƒå¯¹é½å¤„ç†å®Œæˆï¼")
    
    def generate_report(self, processing_report, success_count, total_processed):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        report_path = self.output_dir / "dl_processing_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# æ·±åº¦å­¦ä¹ å›¾åƒå¯¹é½å¤„ç†æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ä½¿ç”¨æ–¹æ³•**: {self.method}\n")
            f.write(f"**ä½¿ç”¨è®¾å¤‡**: {self.device}\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»å›¾åƒæ•°é‡**: {total_processed}\n")
            f.write(f"- **æˆåŠŸå¯¹é½**: {success_count} ({success_count/total_processed*100:.1f}%)\n")
            f.write(f"- **å¤±è´¥æ•°é‡**: {total_processed - success_count}\n\n")
            
            # æ€§èƒ½ç»Ÿè®¡
            if processing_report:
                avg_time = np.mean([r['processing_time'] for r in processing_report])
                avg_matches = np.mean([r['match_points'] for r in processing_report if r['success']])
                avg_inliers = np.mean([r['inliers'] for r in processing_report if r['success']])
                
                f.write("## ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡\n\n")
                f.write(f"- **å¹³å‡å¤„ç†æ—¶é—´**: {avg_time:.2f}ç§’\n")
                f.write(f"- **å¹³å‡åŒ¹é…ç‚¹æ•°**: {avg_matches:.1f}\n")
                f.write(f"- **å¹³å‡å†…ç‚¹æ•°**: {avg_inliers:.1f}\n\n")
            
            # è¯¦ç»†è®°å½•
            f.write("## ğŸ“‹ è¯¦ç»†å¤„ç†è®°å½•\n\n")
            f.write("| æ–‡ä»¶å | åŒ¹é…ç‚¹æ•° | å†…ç‚¹æ•° | è€—æ—¶(ç§’) | çŠ¶æ€ |\n")
            f.write("|--------|----------|--------|----------|------|\n")
            
            for entry in processing_report:
                status = "âœ… æˆåŠŸ" if entry['success'] else "âŒ å¤±è´¥"
                f.write(f"| {entry['filename']} | {entry['match_points']} | {entry['inliers']} | ")
                f.write(f"{entry['processing_time']:.2f} | {status} |\n")
            
            f.write(f"\n---\n*æŠ¥å‘Šç”Ÿæˆäº Deep Learning Align Library*\n")
        
        logger.info(f"å¤„ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Deep Learning Image Alignment')
    parser.add_argument('--input', '-i', default='NPU-Everyday-Sample', 
                       help='è¾“å…¥å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--output', '-o', default='NPU-Everyday-Sample_DL-Align', 
                       help='è¾“å‡ºå›¾åƒæ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--reference', '-r', type=int, default=0, 
                       help='å‚è€ƒå›¾åƒç´¢å¼•')
    parser.add_argument('--align-method', '-m', default='lightweight',
                       choices=['superpoint', 'loftr', 'sift_dl', 'lightweight'],
                       help='æ·±åº¦å­¦ä¹ æ–¹æ³•é€‰æ‹©')
    
    args = parser.parse_args()
    
    print("ğŸš€ Deep Learning Image Alignment")
    print("=" * 60)
    print(f"æ–¹æ³•: {args.align_method}")
    print(f"è¾“å…¥: {args.input}")
    print(f"è¾“å‡º: {args.output}")
    print("=" * 60)
    
    # åˆ›å»ºæ·±åº¦å­¦ä¹ å¯¹é½å™¨
    try:
        aligner = DeepLearningAlign(
            input_dir=args.input,
            output_dir=args.output,
            reference_index=args.reference,
            method=args.align_method
        )
        
        # æ‰§è¡Œå¯¹é½
        aligner.process_images()
        
        print("=" * 60)
        print("âœ… æ·±åº¦å­¦ä¹ å›¾åƒå¯¹é½å®Œæˆï¼")
        print(f"ç»“æœä¿å­˜åœ¨: {args.output}")
        
    except Exception as e:
        logger.error(f"æ·±åº¦å­¦ä¹ å¯¹é½å¤±è´¥: {e}")
        print("âŒ å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—")


if __name__ == "__main__":
    main()