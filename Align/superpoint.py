#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TickTock-SuperPoint-Align Library

åŸºäºSuperPointçš„å›¾åƒå¯¹é½åº“ï¼Œä¸“é—¨é’ˆå¯¹å»ºç­‘ç‰©æ—¶é—´åºåˆ—å›¾åƒã€‚
ä½¿ç”¨LoFTRå’ŒKornia SIFTè¿›è¡Œç°ä»£æ·±åº¦å­¦ä¹ ç‰¹å¾åŒ¹é…ã€‚

åŠŸèƒ½ç‰¹ç‚¹:
- åŸºäºSuperPointçš„ç‰¹å¾æå–å’ŒåŒ¹é…
- ä¼˜å…ˆä½¿ç”¨LoFTRï¼Œå›é€€åˆ°Kornia SIFT
- å¯¹å…‰ç…§å˜åŒ–ã€å­£èŠ‚å˜åŒ–é²æ£’
- æ”¯æŒæ—¥å¤œå›¾åƒçš„ç»Ÿä¸€å¤„ç†
- è‡ªåŠ¨å›é€€åˆ°ä¼ ç»ŸSIFTæ–¹æ³•
- ç®€åŒ–çš„APIï¼Œä¸“æ³¨äºSuperPointåŠŸèƒ½
"""

import cv2
import numpy as np
from pathlib import Path
import logging
import time
import warnings
import requests
from tqdm import tqdm
warnings.filterwarnings('ignore')

# æ·±åº¦å­¦ä¹ å¯¼å…¥
import torch
import torch.nn.functional as F
TORCH_AVAILABLE = True

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "3"  # æŒ‡å®šä½¿ç”¨çš„GPU

# å¯é€‰çš„ç‰¹æ®Šåº“å¯¼å…¥
try:
    import kornia as K
    import kornia.feature as KF
    # å°è¯•å¯¼å…¥å¯ç”¨çš„ç‰¹å¾æ£€æµ‹å™¨
    KORNIA_AVAILABLE = True
    
    # æ£€æŸ¥å¯ç”¨çš„ç‰¹å¾æ£€æµ‹å™¨
    SIFT_AVAILABLE = hasattr(KF, 'SIFTFeature')
    LOFTR_AVAILABLE = hasattr(KF, 'LoFTR')
    
except ImportError:
    KORNIA_AVAILABLE = False
    SIFT_AVAILABLE = False
    LOFTR_AVAILABLE = False

try:
    from scipy.ndimage import maximum_filter
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DeepLearningAlign:
    """
    åŸºäºSuperPointçš„å›¾åƒå¯¹é½ç±»
    
    ä½¿ç”¨LoFTRå’ŒKornia SIFTè¿›è¡Œç‰¹å¾åŒ¹é…ï¼Œä¸“é—¨é’ˆå¯¹å»ºç­‘ç‰©æ—¶é—´åºåˆ—å›¾åƒå¯¹é½ã€‚
    å½“æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸å¯ç”¨æ—¶è‡ªåŠ¨å›é€€åˆ°ä¼ ç»ŸSIFTæ–¹æ³•ã€‚
    """
    
    def __init__(self, input_dir="Lib", output_dir="DL-Align", reference_index=0):
        """
        åˆå§‹åŒ–SuperPointå¯¹é½å™¨
        
        Args:
            input_dir (str): è¾“å…¥å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            output_dir (str): è¾“å‡ºå¯¹é½å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            reference_index (int): å‚è€ƒå›¾åƒç´¢å¼•
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.reference_index = reference_index
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        if TORCH_AVAILABLE:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            if torch.cuda.is_available():
                logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
                logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        else:
            self.device = 'cpu'
            logger.warning("PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPUå’Œä¼ ç»Ÿæ–¹æ³•")
        
        # åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹
        self.init_models()
        
    def init_models(self):
        """åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ - ä»…æ”¯æŒ superpoint"""
        logger.info(f"åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹: superpoint")
        self.init_superpoint()
    
    def init_superpoint(self):
        """åˆå§‹åŒ–Korniaç‰¹å¾æ£€æµ‹å™¨ï¼ˆä¼˜å…ˆä½¿ç”¨LoFTRï¼Œå›é€€åˆ°SIFTï¼‰"""
        if KORNIA_AVAILABLE and TORCH_AVAILABLE:
            try:
                # é¦–å…ˆå°è¯•ä½¿ç”¨æœ¬åœ°LoFTRæ¨¡å‹
                local_loftr_path = "/mnt/houjinliang/MyDevProject/TickTock-NPUEveryday/Align/loftr_outdoor.ckpt"
                
                # æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰LoFTRæ¨¡å‹æ–‡ä»¶
                if LOFTR_AVAILABLE:
                    if not Path(local_loftr_path).exists():
                        logger.info("æœ¬åœ°LoFTRæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
                        success = self.download_loftr_model(local_loftr_path)
                        if not success:
                            logger.warning("LoFTRæ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°å…¶ä»–æ–¹æ³•")
                            self.init_fallback_method()
                            return
                    
                    # ä½¿ç”¨æœ¬åœ°LoFTRæ¨¡å‹
                    import torch
                    state_dict = torch.load(local_loftr_path, map_location=self.device)
                    self.loftr_matcher = KF.LoFTR(pretrained=None)
                    self.loftr_matcher.load_state_dict(state_dict['state_dict'])
                    self.loftr_matcher = self.loftr_matcher.to(self.device).eval()
                    logger.info("LoFTRæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
                    self.model_available = True
                    self.use_loftr = True
                    return
                else:
                    logger.warning("æœªæ‰¾åˆ°LoFTRæ”¯æŒï¼Œå›é€€åˆ°å…¶ä»–æ–¹æ³•")
                    self.init_fallback_method()
                    return
            except Exception as e:
                logger.warning(f"Korniaç‰¹å¾æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        logger.warning("Korniaä¸å¯ç”¨ï¼Œä½¿ç”¨è½»é‡çº§ç‰¹å¾æå–å™¨")
        self.init_lightweight_features()
    
    def download_loftr_model(self, local_path):
        """ä¸‹è½½LoFTRæ¨¡å‹æ–‡ä»¶"""
        try:
            # LoFTR outdooræ¨¡å‹ä¸‹è½½é“¾æ¥
            model_url = "https://github.com/zju3dv/LoFTR/releases/download/v1.0/outdoor_ds.ckpt"
            
            logger.info(f"æ­£åœ¨ä¸‹è½½LoFTRæ¨¡å‹åˆ°: {local_path}")
            
            # åˆ›å»ºç›®å½•
            Path(local_path).parent.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(model_url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(local_path, 'wb') as f, tqdm(
                desc="ä¸‹è½½LoFTRæ¨¡å‹",
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024,
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
            
            logger.info(f"LoFTRæ¨¡å‹ä¸‹è½½å®Œæˆ: {local_path}")
            return True
            
        except Exception as e:
            logger.error(f"LoFTRæ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            # æ¸…ç†å¯èƒ½çš„éƒ¨åˆ†ä¸‹è½½æ–‡ä»¶
            if Path(local_path).exists():
                Path(local_path).unlink()
            return False
    
    def init_fallback_method(self):
        """åˆå§‹åŒ–å›é€€æ–¹æ³•"""
        if SIFT_AVAILABLE:
            # å›é€€åˆ°Kornia SIFTç‰¹å¾
            self.kornia_sift = KF.SIFTFeature(num_features=2000).to(self.device).eval()
            logger.info("Kornia SIFTæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            self.model_available = True
            self.use_loftr = False
        else:
            logger.warning("å›é€€åˆ°è½»é‡çº§ç‰¹å¾æå–å™¨")
            self.init_lightweight_features()
    

    

    

    

    
    def init_traditional_sift(self):
        """åˆå§‹åŒ–ä¼ ç»ŸSIFTä½œä¸ºå›é€€æ–¹æ¡ˆ"""
        self.sift = cv2.SIFT_create(nfeatures=2000)

        logger.info("ä¼ ç»ŸSIFTåˆå§‹åŒ–æˆåŠŸï¼ˆå›é€€æ–¹æ¡ˆï¼‰")
        self.model_available = True
    
    def get_image_files(self):
        """è·å–è¾“å…¥ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒæ–‡ä»¶"""
        image_files = []
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        
        for ext in image_extensions:
            image_files.extend(list(self.input_dir.rglob(f"*{ext}")))
            image_files.extend(list(self.input_dir.rglob(f"*{ext.upper()}")))
        
        image_files = list(set([str(f) for f in image_files]))
        image_files.sort()
        return image_files
    

    
    def extract_features_kornia(self, img):
        """ä½¿ç”¨Korniaç‰¹å¾æ£€æµ‹å™¨æå–ç‰¹å¾"""
        try:
            if hasattr(self, 'use_loftr') and self.use_loftr:
                # å¯¹äºLoFTRï¼Œè¿”å›é¢„å¤„ç†åçš„tensor
                tensor_result, scale, bbox = self.preprocess_for_loftr(img)
                logger.info("LoFTRç‰¹å¾å‡†å¤‡å®Œæˆ")
                return None, tensor_result  # è¿”å›Noneå’Œtensorä¾›åç»­LoFTRåŒ¹é…ä½¿ç”¨
                    
            elif hasattr(self, 'kornia_sift'):
                # é¢„å¤„ç†å›¾åƒ
                if len(img.shape) == 3:
                    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                else:
                    gray = img
                    
                # è½¬æ¢ä¸ºtensor
                img_tensor = torch.from_numpy(gray).float().unsqueeze(0).unsqueeze(0).to(self.device) / 255.0
                
                # ä½¿ç”¨Kornia SIFT
                lafs, responses, descriptors = self.kornia_sift(img_tensor)
                
                if lafs.shape[1] > 0:
                    # è½¬æ¢LAFsåˆ°å…³é”®ç‚¹
                    lafs_np = lafs[0].cpu().numpy()  # [N, 2, 3]
                    keypoints = []
                    
                    for laf in lafs_np:
                        # LAFæ ¼å¼è½¬æ¢ä¸ºå…³é”®ç‚¹åæ ‡
                        x, y = laf[0, 2], laf[1, 2]  # ä¸­å¿ƒåæ ‡
                        # è®¡ç®—å°ºåº¦
                        scale = np.sqrt(laf[0, 0]**2 + laf[0, 1]**2)
                        keypoints.append(cv2.KeyPoint(x=float(x), y=float(y), size=float(scale)))
                    
                    desc_np = descriptors[0].cpu().numpy().T  # [N, 128]
                    
                    logger.info(f"Kornia SIFTç‰¹å¾æå–: {len(keypoints)}ä¸ªå…³é”®ç‚¹")
                    return keypoints, desc_np
                else:
                    logger.warning("Kornia SIFTæœªæ£€æµ‹åˆ°ç‰¹å¾ç‚¹")
                    return self.extract_features_sift(img)
                        
        except Exception as e:
            logger.error(f"Korniaç‰¹å¾æå–å¤±è´¥: {e}")
            return self.extract_features_sift(img)
    

    
    def extract_features_sift(self, img):
        """ä½¿ç”¨ä¼ ç»ŸSIFTæå–ç‰¹å¾"""
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img
            
        keypoints, descriptors = self.sift.detectAndCompute(gray, None)
        return keypoints, descriptors
    
    def extract_features(self, img):
        """æå–ç‰¹å¾ - ä»…æ”¯æŒ superpoint"""
        if hasattr(self, 'loftr_matcher') or hasattr(self, 'kornia_sift'):
            return self.extract_features_kornia(img)
        else:
            return self.extract_features_sift(img)
    
    def preprocess_for_loftr(self, img, target_size=640):
        """ä¸“ä¸ºLoFTRä¼˜åŒ–çš„å›¾åƒé¢„å¤„ç†"""
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img
        
        # è·å–åŸå§‹å°ºå¯¸
        h, w = gray.shape
        
        # è®¡ç®—ç¼©æ”¾å› å­ï¼Œä¿æŒçºµæ¨ªæ¯”
        scale = min(target_size / w, target_size / h)
        new_w, new_h = int(w * scale), int(h * scale)
        
        # è°ƒæ•´å¤§å°
        resized = cv2.resize(gray, (new_w, new_h))
        
        # åˆ›å»ºå¡«å……çš„æ­£æ–¹å½¢å›¾åƒ
        padded = np.zeros((target_size, target_size), dtype=np.uint8)
        start_x = (target_size - new_w) // 2
        start_y = (target_size - new_h) // 2
        padded[start_y:start_y+new_h, start_x:start_x+new_w] = resized
        
        # è½¬æ¢ä¸ºtensor
        tensor_img = torch.from_numpy(padded).float().unsqueeze(0).unsqueeze(0).to(self.device) / 255.0
        
        return tensor_img, scale, (start_x, start_y, new_w, new_h)

    def match_features_loftr(self, ref_tensor_info, curr_img):
        """ä½¿ç”¨LoFTRè¿›è¡Œç‰¹å¾åŒ¹é… - ä¼˜åŒ–ç‰ˆ"""
        try:
            # é¢„å¤„ç†å½“å‰å›¾åƒ
            curr_tensor, curr_scale, (curr_sx, curr_sy, curr_w, curr_h) = self.preprocess_for_loftr(curr_img)
            
            # å¼•ç”¨å›¾åƒä¿¡æ¯ (ref_tensor_info å°±æ˜¯é¢„å¤„ç†åçš„tensor)
            ref_tensor = ref_tensor_info
            
            with torch.no_grad():
                # å‡†å¤‡è¾“å…¥æ•°æ®
                input_dict = {
                    'image0': ref_tensor,    # [1, 1, H, W]
                    'image1': curr_tensor    # [1, 1, H, W]
                }
                
                # è¿è¡ŒLoFTR
                correspondences = self.loftr_matcher(input_dict)
                
                # æå–åŒ¹é…ç»“æœ
                mkpts0 = correspondences['keypoints0'].cpu().numpy()  # [N, 2]
                mkpts1 = correspondences['keypoints1'].cpu().numpy()  # [N, 2]
                mconf = correspondences['confidence'].cpu().numpy()   # [N]
                
                # ä½¿ç”¨æ›´ä½çš„ç½®ä¿¡åº¦é˜ˆå€¼
                confidence_thresh = 0.1
                mask = mconf > confidence_thresh
                mkpts0_filtered = mkpts0[mask]
                mkpts1_filtered = mkpts1[mask]
                mconf_filtered = mconf[mask]
                
                if len(mkpts0_filtered) == 0:
                    logger.warning("âš ï¸  æ²¡æœ‰è¶³å¤Ÿç½®ä¿¡åº¦çš„åŒ¹é…ç‚¹")
                    return [], [], []
                
                # å°†åæ ‡ä»å¡«å……å›¾åƒè½¬æ¢å›åŸå§‹å›¾åƒåæ ‡
                # å‚è€ƒå›¾åƒåæ ‡è½¬æ¢ (å‡è®¾ä½¿ç”¨ç›¸åŒçš„é¢„å¤„ç†)
                ref_scale = curr_scale  # å‡è®¾å‚è€ƒå›¾åƒç”¨ç›¸åŒé¢„å¤„ç†
                mkpts0_orig = mkpts0_filtered.copy()
                mkpts0_orig[:, 0] = (mkpts0_orig[:, 0] - curr_sx) / ref_scale
                mkpts0_orig[:, 1] = (mkpts0_orig[:, 1] - curr_sy) / ref_scale
                
                # å½“å‰å›¾åƒåæ ‡è½¬æ¢
                mkpts1_orig = mkpts1_filtered.copy()
                mkpts1_orig[:, 0] = (mkpts1_orig[:, 0] - curr_sx) / curr_scale
                mkpts1_orig[:, 1] = (mkpts1_orig[:, 1] - curr_sy) / curr_scale
                
                # è¿‡æ»¤è¶…å‡ºåŸå§‹å›¾åƒè¾¹ç•Œçš„ç‚¹
                ref_h, ref_w = self.reference_shape[:2]
                curr_h, curr_w = curr_img.shape[:2]
                
                valid_mask = ((mkpts0_orig[:, 0] >= 0) & (mkpts0_orig[:, 0] < ref_w) &
                            (mkpts0_orig[:, 1] >= 0) & (mkpts0_orig[:, 1] < ref_h) &
                            (mkpts1_orig[:, 0] >= 0) & (mkpts1_orig[:, 0] < curr_w) &
                            (mkpts1_orig[:, 1] >= 0) & (mkpts1_orig[:, 1] < curr_h))
                
                mkpts0_final = mkpts0_orig[valid_mask]
                mkpts1_final = mkpts1_orig[valid_mask]
                mconf_final = mconf_filtered[valid_mask]
                
                # åˆ›å»ºOpenCVåŒ¹é…æ ¼å¼
                matches = []
                kp1_list = []
                kp2_list = []
                
                for i in range(len(mkpts0_final)):
                    kp1_list.append(cv2.KeyPoint(x=mkpts0_final[i, 0], y=mkpts0_final[i, 1], size=1))
                    kp2_list.append(cv2.KeyPoint(x=mkpts1_final[i, 0], y=mkpts1_final[i, 1], size=1))
                    matches.append(cv2.DMatch(i, i, float(1.0 - mconf_final[i])))
                
                logger.info(f"LoFTRæ‰¾åˆ° {len(matches)} ä¸ªæœ‰æ•ˆåŒ¹é…")
                return matches, kp1_list, kp2_list
                
        except Exception as e:
            logger.error(f"LoFTRåŒ¹é…å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [], [], []
    

    
    def match_features_traditional(self, desc1, desc2):
        """ä¼ ç»Ÿç‰¹å¾åŒ¹é…"""
        # ä½¿ç”¨FLANNåŒ¹é…å™¨
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        search_params = dict(checks=50)
        
        flann = cv2.FlannBasedMatcher(index_params, search_params)
        
        try:
            matches = flann.knnMatch(desc1, desc2, k=2)
            
            # Lowe's ratio test
            good_matches = []
            for match_pair in matches:
                if len(match_pair) == 2:
                    m, n = match_pair
                    if m.distance < 0.7 * n.distance:
                        good_matches.append(m)
            
            return good_matches
            
        except Exception as e:
            logger.warning(f"FLANNåŒ¹é…å¤±è´¥: {e}")
            return []
    
    def estimate_homography_robust(self, kp1, kp2, matches, ransac_thresh=5.0):
        """é²æ£’çš„å•åº”æ€§çŸ©é˜µä¼°è®¡"""
        if len(matches) < 4:  # OpenCVæœ€ä½è¦æ±‚æ˜¯4ä¸ªç‚¹
            logger.warning(f"åŒ¹é…ç‚¹æ•°é‡ä¸è¶³ ({len(matches)})ï¼Œæ— æ³•è®¡ç®—å•åº”æ€§çŸ©é˜µ")
            return None, 0
        
        # æå–åŒ¹é…ç‚¹åæ ‡
        src_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
        
        # ä½¿ç”¨RANSACä¼°è®¡å•åº”æ€§çŸ©é˜µï¼Œä¼˜åŒ–å‚æ•°
        homography, mask = cv2.findHomography(
            src_pts, dst_pts, 
            cv2.RANSAC, 
            ransacReprojThreshold=ransac_thresh,  # æ›´å®½æ¾çš„é˜ˆå€¼
            maxIters=10000,  # æ›´å¤šè¿­ä»£
            confidence=0.995  # æ›´é«˜ç½®ä¿¡åº¦
        )
        
        inliers = np.sum(mask) if mask is not None else 0
        
        # æ£€æŸ¥å•åº”æ€§çŸ©é˜µè´¨é‡
        if homography is not None:
            # æ£€æŸ¥æ¡ä»¶æ•°
            cond_num = np.linalg.cond(homography)
            if cond_num > 100000:  # æ¡ä»¶æ•°è¿‡é«˜
                logger.warning(f"å•åº”æ€§çŸ©é˜µæ¡ä»¶æ•°è¿‡é«˜: {cond_num:.0f}ï¼Œå°è¯•é™ä½ç²¾åº¦è¦æ±‚")
                # å°è¯•æ›´å®½æ¾çš„å‚æ•°
                homography, mask = cv2.findHomography(
                    src_pts, dst_pts, 
                    cv2.RANSAC, 
                    ransacReprojThreshold=ransac_thresh * 2,  # æ›´å®½æ¾
                    maxIters=5000,
                    confidence=0.99
                )
                inliers = np.sum(mask) if mask is not None else 0
        
        return homography, inliers
    
    def align_image(self, img, homography, reference_shape):
        """å¯¹é½å›¾åƒ"""
        if homography is None:
            logger.warning("å•åº”æ€§çŸ©é˜µä¸ºç©ºï¼Œè¿”å›è°ƒæ•´å¤§å°åçš„åŸå›¾åƒ")
            return cv2.resize(img, (reference_shape[1], reference_shape[0]))
        
        aligned_img = cv2.warpPerspective(
            img, homography, 
            (reference_shape[1], reference_shape[0]),
            flags=cv2.INTER_LINEAR,
            borderMode=cv2.BORDER_CONSTANT,
            borderValue=(0, 0, 0)
        )
        
        return aligned_img
    
    def process_images(self):
        """å¤„ç†æ‰€æœ‰å›¾åƒè¿›è¡Œå¯¹é½"""
        image_files = self.get_image_files()
        
        if not image_files:
            logger.error(f"åœ¨ {self.input_dir} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            return
        
        logger.info(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
        
        # è¯»å–å‚è€ƒå›¾åƒ
        if self.reference_index >= len(image_files):
            logger.error(f"å‚è€ƒå›¾åƒç´¢å¼• {self.reference_index} è¶…å‡ºèŒƒå›´")
            return
        
        reference_path = image_files[self.reference_index]
        reference_img = cv2.imread(reference_path)
        
        if reference_img is None:
            logger.error(f"æ— æ³•è¯»å–å‚è€ƒå›¾åƒ: {reference_path}")
            return
        
        logger.info(f"ä½¿ç”¨å‚è€ƒå›¾åƒ: {Path(reference_path).name}")
        
        # ä¿å­˜å‚è€ƒå›¾åƒå°ºå¯¸ä¾›LoFTRä½¿ç”¨
        self.reference_shape = reference_img.shape
        
        # æå–å‚è€ƒå›¾åƒç‰¹å¾
        ref_kp, ref_desc = self.extract_features(reference_img)
        
        # ç‰¹æ®Šå¤„ç†LoFTRæƒ…å†µ
        if hasattr(self, 'use_loftr') and self.use_loftr:
            if ref_desc is None:
                logger.error("å‚è€ƒå›¾åƒtensoræå–å¤±è´¥")
                return
            logger.info("å‚è€ƒå›¾åƒå·²å‡†å¤‡ç”¨äºLoFTRåŒ¹é…")
        else:
            if ref_desc is None:
                logger.error("å‚è€ƒå›¾åƒç‰¹å¾æå–å¤±è´¥")
                return
            logger.info(f"å‚è€ƒå›¾åƒæå–åˆ° {len(ref_kp)} ä¸ªç‰¹å¾ç‚¹")
        
        # ä¿å­˜å‚è€ƒå›¾åƒ
        ref_output_path = self.output_dir / Path(reference_path).name
        cv2.imwrite(str(ref_output_path), reference_img)
        logger.info(f"ä¿å­˜å‚è€ƒå›¾åƒ: {ref_output_path}")
        
        # å¤„ç†ç»Ÿè®¡
        success_count = 0
        total_processed = 0
        processing_report = []
        
        # å¤„ç†å…¶ä»–å›¾åƒ
        for i, img_path in enumerate(image_files):
            if i == self.reference_index:
                continue
            
            logger.info(f"å¤„ç†å›¾åƒ {i+1}/{len(image_files)}: {Path(img_path).name}")
            start_time = time.time()
            
            # è¯»å–å½“å‰å›¾åƒ
            current_img = cv2.imread(img_path)
            if current_img is None:
                logger.warning(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")
                continue
            
            total_processed += 1
            
            # superpoint ç‰¹å¾æå–å’ŒåŒ¹é…
            homography = None
            match_points = 0
            inliers = 0
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨LoFTR
            if hasattr(self, 'use_loftr') and self.use_loftr and hasattr(self, 'loftr_matcher'):
                # LoFTRç›´æ¥åŒ¹é…ä¸¤å¼ å›¾åƒ
                matches, matched_kp1, matched_kp2 = self.match_features_loftr(ref_desc, current_img)
                match_points = len(matches)
                
                logger.info(f"LoFTRæ‰¾åˆ° {match_points} ä¸ªåŒ¹é…ç‚¹")
                
                if match_points >= 4:
                    # ä½¿ç”¨æ›´å®½æ¾çš„RANSACå‚æ•°
                    homography, inliers = self.estimate_homography_robust(matched_kp1, matched_kp2, matches, ransac_thresh=8.0)
                    
                    if homography is not None:
                        logger.info(f"LoFTRå¯¹é½æˆåŠŸï¼Œå†…ç‚¹æ•°é‡: {inliers}")
                    else:
                        logger.warning("LoFTRå¯¹é½å¤±è´¥")
                else:
                    homography = None
                    inliers = 0
                    logger.warning("LoFTRåŒ¹é…ç‚¹ä¸è¶³")
            else:
                # ä¼ ç»Ÿçš„ Kornia SIFT ç‰¹å¾æå–å’ŒåŒ¹é…
                curr_kp, curr_desc = self.extract_features(current_img)
                
                if curr_desc is not None:
                    # åŒ¹é…ç‰¹å¾ç‚¹
                    matches = self.match_features_traditional(ref_desc, curr_desc)
                    match_points = len(matches)
                    
                    logger.info(f"æ‰¾åˆ° {match_points} ä¸ªåŒ¹é…ç‚¹")
                    
                    # ä¼°è®¡å•åº”æ€§çŸ©é˜µ
                    homography, inliers = self.estimate_homography_robust(ref_kp, curr_kp, matches)
                    
                    if homography is not None:
                        logger.info(f"superpointå¯¹é½æˆåŠŸï¼Œå†…ç‚¹æ•°é‡: {inliers}")
                    else:
                        logger.warning("superpointå¯¹é½å¤±è´¥")
            
            # å¯¹é½å›¾åƒ
            aligned_img = self.align_image(current_img, homography, reference_img.shape)
            
            # ä¿å­˜å¯¹é½åçš„å›¾åƒ
            output_path = self.output_dir / Path(img_path).name
            cv2.imwrite(str(output_path), aligned_img)
            
            processing_time = time.time() - start_time
            success = homography is not None
            
            if success:
                success_count += 1
            
            # è®°å½•å¤„ç†æŠ¥å‘Š
            report_entry = {
                'filename': Path(img_path).name,
                'method': 'superpoint',
                'match_points': match_points,
                'inliers': inliers,
                'processing_time': processing_time,
                'success': success
            }
            processing_report.append(report_entry)
            
            logger.info(f"ä¿å­˜å¯¹é½å›¾åƒ: {output_path} (æ·±åº¦å­¦ä¹ , {processing_time:.2f}ç§’)")
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        logger.info("=" * 60)
        logger.info("SuperPointå¯¹é½å¤„ç†ç»Ÿè®¡:")
        logger.info(f"æ€»å›¾åƒæ•°é‡: {total_processed}")
        logger.info(f"æˆåŠŸå¯¹é½: {success_count}")
        logger.info(f"æˆåŠŸç‡: {success_count/total_processed*100:.1f}%")
        logger.info(f"ä½¿ç”¨æ–¹æ³•: superpoint")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        self.generate_report(processing_report, success_count, total_processed)
        
        logger.info("SuperPointå›¾åƒå¯¹é½å¤„ç†å®Œæˆï¼")
    
    def generate_report(self, processing_report, success_count, total_processed):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        report_path = self.output_dir / "superpoint_processing_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# SuperPointå›¾åƒå¯¹é½å¤„ç†æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ä½¿ç”¨æ–¹æ³•**: superpoint\n")
            f.write(f"**ä½¿ç”¨è®¾å¤‡**: {self.device}\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»å›¾åƒæ•°é‡**: {total_processed}\n")
            f.write(f"- **æˆåŠŸå¯¹é½**: {success_count} ({success_count/total_processed*100:.1f}%)\n")
            f.write(f"- **å¤±è´¥æ•°é‡**: {total_processed - success_count}\n\n")
            
            # æ€§èƒ½ç»Ÿè®¡
            if processing_report:
                avg_time = np.mean([r['processing_time'] for r in processing_report])
                avg_matches = np.mean([r['match_points'] for r in processing_report if r['success']])
                avg_inliers = np.mean([r['inliers'] for r in processing_report if r['success']])
                
                f.write("## ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡\n\n")
                f.write(f"- **å¹³å‡å¤„ç†æ—¶é—´**: {avg_time:.2f}ç§’\n")
                f.write(f"- **å¹³å‡åŒ¹é…ç‚¹æ•°**: {avg_matches:.1f}\n")
                f.write(f"- **å¹³å‡å†…ç‚¹æ•°**: {avg_inliers:.1f}\n\n")
            
            # è¯¦ç»†è®°å½•
            f.write("## ğŸ“‹ è¯¦ç»†å¤„ç†è®°å½•\n\n")
            f.write("| æ–‡ä»¶å | åŒ¹é…ç‚¹æ•° | å†…ç‚¹æ•° | è€—æ—¶(ç§’) | çŠ¶æ€ |\n")
            f.write("|--------|----------|--------|----------|------|\n")
            
            for entry in processing_report:
                status = "âœ… æˆåŠŸ" if entry['success'] else "âŒ å¤±è´¥"
                f.write(f"| {entry['filename']} | {entry['match_points']} | {entry['inliers']} | ")
                f.write(f"{entry['processing_time']:.2f} | {status} |\n")
            
            f.write(f"\n---\n*æŠ¥å‘Šç”Ÿæˆäº Deep Learning Align Library*\n")
        
        logger.info(f"å¤„ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

