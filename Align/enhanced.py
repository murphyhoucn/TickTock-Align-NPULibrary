#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TickTock-Enhanced-Align-NPU Library

è¿™ä¸ªå¢å¼ºç‰ˆå¯¹é½åº“ä¸“é—¨é’ˆå¯¹æ—¥å¤œæ··åˆçš„å›¾åƒåºåˆ—è¿›è¡Œä¼˜åŒ–ã€‚
è§£å†³äº†ä¼ ç»ŸSIFTç®—æ³•åœ¨å¤œé—´ç…§ç‰‡ä¸Šæ•ˆæœä¸ä½³çš„é—®é¢˜ã€‚

åŠŸèƒ½ç‰¹ç‚¹:
- è‡ªåŠ¨æ£€æµ‹æ—¥å¤œå›¾åƒå¹¶é‡‡ç”¨ä¸åŒç­–ç•¥
- å¤œé—´å›¾åƒä½¿ç”¨å¢å¼ºé¢„å¤„ç† + å¤šç§ç‰¹å¾æ£€æµ‹å™¨ç»„åˆ
- ç™½å¤©å›¾åƒä½¿ç”¨ä¼˜åŒ–çš„SIFTç®—æ³•
- æ”¯æŒæ¨¡æ¿åŒ¹é…ä½œä¸ºåå¤‡æ–¹æ¡ˆ
- æ¸è¿›å¼å¯¹é½ç­–ç•¥
"""

import cv2
import numpy as np
import os
import glob
from pathlib import Path
import logging
from typing import Tuple, List, Optional
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedAlign:
    """
    å¢å¼ºç‰ˆ TickTock-Align-NPU å›¾åƒå¯¹é½ç±»
    
    ä¸“é—¨ä¼˜åŒ–å¤„ç†æ—¥å¤œæ··åˆçš„å»ºç­‘ç‰©å›¾åƒåºåˆ—ï¼Œç¡®ä¿å¤œé—´ç…§ç‰‡ä¹Ÿèƒ½è·å¾—è‰¯å¥½çš„å¯¹é½æ•ˆæœã€‚
    """
    
    def __init__(self, input_dir="Lib", output_dir="Enhanced-Align", reference_index=0):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆå¯¹é½å™¨
        
        Args:
            input_dir (str): è¾“å…¥å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            output_dir (str): è¾“å‡ºå¯¹é½å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            reference_index (int): å‚è€ƒå›¾åƒç´¢å¼•ï¼ˆé»˜è®¤ä¸º0ï¼Œå³ç¬¬ä¸€å¼ å›¾åƒï¼‰
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.reference_index = reference_index
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True)
        
        # æ”¯æŒçš„å›¾åƒæ ¼å¼
        self.supported_formats = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']
        
        # æ—¥å¤œåˆ¤æ–­é˜ˆå€¼
        self.night_threshold = 80  # å¹³å‡äº®åº¦ä½äºæ­¤å€¼è®¤ä¸ºæ˜¯å¤œé—´å›¾åƒ
        
        # ç‰¹å¾æ£€æµ‹å™¨å‚æ•°
        self.init_feature_detectors()
        
    def init_feature_detectors(self):
        """åˆå§‹åŒ–å¤šç§ç‰¹å¾æ£€æµ‹å™¨"""
        # SIFTæ£€æµ‹å™¨ - é€‚ç”¨äºç™½å¤©å›¾åƒ
        self.sift = cv2.SIFT_create(nfeatures=1000, contrastThreshold=0.04, edgeThreshold=10)
        
        # ORBæ£€æµ‹å™¨ - å¯¹å…‰ç…§å˜åŒ–é²æ£’
        self.orb = cv2.ORB_create(nfeatures=1500, scaleFactor=1.2, nlevels=8)
        
        # AKAZEæ£€æµ‹å™¨ - å¯¹å™ªå£°é²æ£’
        self.akaze = cv2.AKAZE_create()
        
        # BRISKæ£€æµ‹å™¨ - å¿«é€Ÿä¸”é²æ£’
        self.brisk = cv2.BRISK_create()
        
    def get_image_files(self):
        """è·å–è¾“å…¥ç›®å½•ä¸­çš„æ‰€æœ‰å›¾åƒæ–‡ä»¶ï¼ˆé€’å½’æœç´¢å­ç›®å½•ï¼‰"""
        image_files = []
        
        # ä½¿ç”¨pathlibçš„rglobè¿›è¡Œé€’å½’æœç´¢
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        for ext in image_extensions:
            # æœç´¢å°å†™æ‰©å±•å
            image_files.extend(list(self.input_dir.rglob(f"*{ext}")))
            # æœç´¢å¤§å†™æ‰©å±•å
            image_files.extend(list(self.input_dir.rglob(f"*{ext.upper()}")))
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²è·¯å¾„å¹¶å»é‡
        image_files = list(set([str(f) for f in image_files]))
        
        # æŒ‰æ–‡ä»¶åæ’åº
        image_files.sort()
        return image_files
    
    def is_night_image(self, img) -> bool:
        """
        åˆ¤æ–­å›¾åƒæ˜¯å¦ä¸ºå¤œé—´æ‹æ‘„
        
        Args:
            img: è¾“å…¥å›¾åƒ
            
        Returns:
            bool: Trueè¡¨ç¤ºå¤œé—´å›¾åƒï¼ŒFalseè¡¨ç¤ºç™½å¤©å›¾åƒ
        """
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img
        
        # è®¡ç®—å¹³å‡äº®åº¦
        mean_brightness = np.mean(gray)
        
        # è®¡ç®—äº®åº¦æ–¹å·®ï¼ˆå¤œé—´å›¾åƒé€šå¸¸å¯¹æ¯”åº¦æ›´é«˜ï¼‰
        brightness_std = np.std(gray)
        
        # ç»¼åˆåˆ¤æ–­ï¼šå¹³å‡äº®åº¦ä½ä¸”æ–¹å·®å¤§çš„å›¾åƒå¯èƒ½æ˜¯å¤œé—´å›¾åƒ
        is_night = mean_brightness < self.night_threshold
        
        logger.debug(f"å›¾åƒäº®åº¦åˆ†æ - å¹³å‡å€¼: {mean_brightness:.2f}, æ ‡å‡†å·®: {brightness_std:.2f}, åˆ¤å®š: {'å¤œé—´' if is_night else 'ç™½å¤©'}")
        
        return is_night
    
    def enhance_night_image(self, img):
        """
        å¢å¼ºå¤œé—´å›¾åƒä»¥æé«˜ç‰¹å¾æ£€æµ‹æ•ˆæœ
        
        Args:
            img: è¾“å…¥å¤œé—´å›¾åƒ
            
        Returns:
            enhanced_img: å¢å¼ºåçš„å›¾åƒ
        """
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img.copy()
        
        # 1. ç›´æ–¹å›¾å‡è¡¡åŒ–
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(gray)
        
        # 2. ä¼½é©¬æ ¡æ­£
        gamma = 1.5  # æäº®æš—éƒ¨
        lookup_table = np.array([((i / 255.0) ** (1.0 / gamma)) * 255
                                for i in np.arange(0, 256)]).astype("uint8")
        enhanced = cv2.LUT(enhanced, lookup_table)
        
        # 3. åŒè¾¹æ»¤æ³¢å»å™ª
        enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
        
        # 4. é”åŒ–å¤„ç†
        kernel = np.array([[-1, -1, -1],
                          [-1,  9, -1],
                          [-1, -1, -1]])
        enhanced = cv2.filter2D(enhanced, -1, kernel)
        
        # ç¡®ä¿å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        enhanced = np.clip(enhanced, 0, 255).astype(np.uint8)
        
        return enhanced
    
    def detect_features_original_sift(self, img):
        """
        åŸå§‹SIFTç‰¹å¾æ£€æµ‹ï¼ˆæ¥è‡ªalign_lib.pyï¼‰
        
        Args:
            img: è¾“å…¥å›¾åƒ
            
        Returns:
            keypoints: ç‰¹å¾ç‚¹
            descriptors: ç‰¹å¾æè¿°ç¬¦
        """
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img
            
        # åˆ›å»ºSIFTæ£€æµ‹å™¨ - ä¿æŒåŸå§‹è®¾ç½®ä»¥è·å–æ›´å¤šç‰¹å¾ç‚¹
        sift = cv2.SIFT_create()
        
        # æ£€æµ‹ç‰¹å¾ç‚¹å’Œæè¿°ç¬¦
        keypoints, descriptors = sift.detectAndCompute(gray, None)
        
        return keypoints, descriptors
    
    def match_features_original(self, desc1, desc2):
        """
        åŸå§‹ç‰¹å¾åŒ¹é…æ–¹æ³•ï¼ˆæ¥è‡ªalign_lib.pyï¼‰
        
        Args:
            desc1: ç¬¬ä¸€å¹…å›¾åƒçš„ç‰¹å¾æè¿°ç¬¦
            desc2: ç¬¬äºŒå¹…å›¾åƒçš„ç‰¹å¾æè¿°ç¬¦
            
        Returns:
            good_matches: è‰¯å¥½çš„åŒ¹é…ç‚¹å¯¹
        """
        if desc1 is None or desc2 is None:
            return []
            
        # ä½¿ç”¨FLANNåŒ¹é…å™¨ - ä¿æŒåŸå§‹è®¾ç½®
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        search_params = dict(checks=50)
        
        flann = cv2.FlannBasedMatcher(index_params, search_params)
        matches = flann.knnMatch(desc1, desc2, k=2)
        
        # åº”ç”¨Lowe's ratio testç­›é€‰è‰¯å¥½åŒ¹é… - ä¿æŒåŸå§‹è®¾ç½®
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < 0.7 * n.distance:
                    good_matches.append(m)
        
        return good_matches
    
    def estimate_homography_original(self, kp1, kp2, matches):
        """
        åŸå§‹å•åº”æ€§çŸ©é˜µä¼°è®¡ï¼ˆæ¥è‡ªalign_lib.pyï¼‰
        
        Args:
            kp1: å‚è€ƒå›¾åƒç‰¹å¾ç‚¹
            kp2: ç›®æ ‡å›¾åƒç‰¹å¾ç‚¹
            matches: åŒ¹é…ç‚¹å¯¹
            
        Returns:
            homography: å•åº”æ€§çŸ©é˜µ
        """
        if len(matches) < 4:
            logger.warning("åŒ¹é…ç‚¹æ•°é‡ä¸è¶³ï¼Œæ— æ³•è®¡ç®—å•åº”æ€§çŸ©é˜µ")
            return None
            
        # æå–åŒ¹é…ç‚¹åæ ‡
        src_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
        
        # ä½¿ç”¨RANSACä¼°è®¡å•åº”æ€§çŸ©é˜µ - ä¿æŒåŸå§‹å‚æ•°ä½†æ·»åŠ è´¨é‡è¯„ä¼°
        homography, mask = cv2.findHomography(
            src_pts, dst_pts, 
            cv2.RANSAC, 
            ransacReprojThreshold=5.0
        )
        
        # è®¡ç®—å†…ç‚¹æ•°é‡ç”¨äºè´¨é‡è¯„ä¼°
        inliers = np.sum(mask) if mask is not None else 0
        if inliers > 0:
            logger.debug(f"å•åº”æ€§ä¼°è®¡: åŒ¹é…ç‚¹{len(matches)}, å†…ç‚¹{inliers}, å†…ç‚¹ç‡{inliers/len(matches)*100:.1f}%")
        
        return homography
    
    def detect_features_adaptive(self, img, is_night=False):
        """
        è‡ªé€‚åº”ç‰¹å¾æ£€æµ‹ï¼šæ ¹æ®å›¾åƒç±»å‹é€‰æ‹©æœ€ä½³æ£€æµ‹ç­–ç•¥
        
        Args:
            img: è¾“å…¥å›¾åƒ
            is_night: æ˜¯å¦ä¸ºå¤œé—´å›¾åƒ
            
        Returns:
            keypoints: ç‰¹å¾ç‚¹
            descriptors: ç‰¹å¾æè¿°ç¬¦
            detector_used: ä½¿ç”¨çš„æ£€æµ‹å™¨åç§°
        """
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(img.shape) == 3:
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            gray = img
        
        if is_night:
            # å¤œé—´å›¾åƒï¼šå…ˆå¢å¼ºå†æ£€æµ‹
            enhanced = self.enhance_night_image(img)
            
            # å°è¯•å¤šç§æ£€æµ‹å™¨å¹¶é€‰æ‹©æœ€ä½³ç»“æœ
            detectors = [
                ("AKAZE", self.akaze),
                ("ORB", self.orb),
                ("BRISK", self.brisk),
                ("SIFT", self.sift)
            ]
            
            best_kp, best_desc, best_detector = None, None, None
            max_features = 0
            
            for detector_name, detector in detectors:
                try:
                    if detector_name in ["AKAZE", "BRISK"]:
                        kp, desc = detector.detectAndCompute(enhanced, None)
                    else:
                        kp, desc = detector.detectAndCompute(enhanced, None)
                    
                    if desc is not None and len(kp) > max_features:
                        max_features = len(kp)
                        best_kp, best_desc, best_detector = kp, desc, detector_name
                        
                except Exception as e:
                    logger.warning(f"{detector_name} æ£€æµ‹å¤±è´¥: {e}")
                    continue
            
            logger.info(f"å¤œé—´å›¾åƒä½¿ç”¨ {best_detector} æ£€æµ‹åˆ° {max_features} ä¸ªç‰¹å¾ç‚¹")
            return best_kp, best_desc, best_detector
        
        else:
            # ç™½å¤©å›¾åƒï¼šä½¿ç”¨ä¼˜åŒ–çš„SIFT
            kp, desc = self.sift.detectAndCompute(gray, None)
            feature_count = len(kp) if kp else 0
            logger.info(f"ç™½å¤©å›¾åƒä½¿ç”¨ SIFT æ£€æµ‹åˆ° {feature_count} ä¸ªç‰¹å¾ç‚¹")
            return kp, desc, "SIFT"
    
    def match_features_robust(self, desc1, desc2, detector1, detector2):
        """
        é²æ£’çš„ç‰¹å¾åŒ¹é…ï¼šæ ¹æ®æ£€æµ‹å™¨ç±»å‹é€‰æ‹©åˆé€‚çš„åŒ¹é…ç­–ç•¥
        
        Args:
            desc1: ç¬¬ä¸€å¹…å›¾åƒçš„ç‰¹å¾æè¿°ç¬¦
            desc2: ç¬¬äºŒå¹…å›¾åƒçš„ç‰¹å¾æè¿°ç¬¦
            detector1: ç¬¬ä¸€å¹…å›¾åƒä½¿ç”¨çš„æ£€æµ‹å™¨
            detector2: ç¬¬äºŒå¹…å›¾åƒä½¿ç”¨çš„æ£€æµ‹å™¨
            
        Returns:
            good_matches: è‰¯å¥½çš„åŒ¹é…ç‚¹å¯¹
        """
        if desc1 is None or desc2 is None:
            return []
        
        # æ£€æŸ¥æè¿°ç¬¦ç±»å‹æ˜¯å¦å…¼å®¹
        binary_detectors = ["ORB", "AKAZE", "BRISK"]
        float_detectors = ["SIFT"]
        
        detector1_is_binary = detector1 in binary_detectors
        detector2_is_binary = detector2 in binary_detectors
        
        # å¦‚æœæè¿°ç¬¦ç±»å‹ä¸åŒ¹é…ï¼Œè¿”å›ç©ºåŒ¹é…ï¼ˆç¨åä½¿ç”¨æ¨¡æ¿åŒ¹é…ï¼‰
        if detector1_is_binary != detector2_is_binary:
            logger.warning(f"æè¿°ç¬¦ç±»å‹ä¸åŒ¹é…: {detector1} vs {detector2}ï¼Œå°†ä½¿ç”¨æ¨¡æ¿åŒ¹é…")
            return []
        
        try:
            # æ ¹æ®æ£€æµ‹å™¨ç±»å‹é€‰æ‹©åŒ¹é…æ–¹æ³•
            if detector1_is_binary and detector2_is_binary:
                # ä¸¤ä¸ªéƒ½æ˜¯äºŒè¿›åˆ¶æè¿°ç¬¦ï¼Œä½¿ç”¨æ±‰æ˜è·ç¦»
                bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)
                matches = bf.knnMatch(desc1, desc2, k=2)
            else:
                # ä¸¤ä¸ªéƒ½æ˜¯æµ®ç‚¹æè¿°ç¬¦ï¼Œä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»å’ŒFLANN
                FLANN_INDEX_KDTREE = 1
                index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
                search_params = dict(checks=50)
                
                flann = cv2.FlannBasedMatcher(index_params, search_params)
                matches = flann.knnMatch(desc1, desc2, k=2)
            
            # åº”ç”¨Lowe's ratio testç­›é€‰è‰¯å¥½åŒ¹é…
            good_matches = []
            ratio_threshold = 0.75  # å¯¹å¤œé—´å›¾åƒæ”¾å®½é˜ˆå€¼
            
            for match_pair in matches:
                if len(match_pair) == 2:
                    m, n = match_pair
                    if m.distance < ratio_threshold * n.distance:
                        good_matches.append(m)
            
            return good_matches
            
        except Exception as e:
            logger.warning(f"ç‰¹å¾åŒ¹é…å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ¨¡æ¿åŒ¹é…")
            return []
    
    def estimate_homography_robust(self, kp1, kp2, matches):
        """
        é²æ£’çš„å•åº”æ€§çŸ©é˜µä¼°è®¡
        
        Args:
            kp1: å‚è€ƒå›¾åƒç‰¹å¾ç‚¹
            kp2: ç›®æ ‡å›¾åƒç‰¹å¾ç‚¹
            matches: åŒ¹é…ç‚¹å¯¹
            
        Returns:
            homography: å•åº”æ€§çŸ©é˜µ
            inliers: å†…ç‚¹æ•°é‡
        """
        if len(matches) < 8:  # æé«˜æœ€å°åŒ¹é…ç‚¹è¦æ±‚
            logger.warning(f"åŒ¹é…ç‚¹æ•°é‡ä¸è¶³ ({len(matches)}ï¼Œéœ€è¦è‡³å°‘8ä¸ª)ï¼Œæ— æ³•è®¡ç®—å•åº”æ€§çŸ©é˜µ")
            return None, 0
            
        # æå–åŒ¹é…ç‚¹åæ ‡
        src_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
        
        # ä½¿ç”¨æ›´ä¸¥æ ¼çš„RANSACå‚æ•°
        homography, mask = cv2.findHomography(
            src_pts, dst_pts, 
            cv2.RANSAC, 
            ransacReprojThreshold=3.0,  # æ›´ä¸¥æ ¼çš„é‡æŠ•å½±è¯¯å·®é˜ˆå€¼
            maxIters=5000,  # å¢åŠ è¿­ä»£æ¬¡æ•°
            confidence=0.995  # æé«˜ç½®ä¿¡åº¦
        )
        
        inliers = np.sum(mask) if mask is not None else 0
        
        return homography, inliers
    
    def template_matching_fallback(self, ref_img, target_img):
        """
        æ¨¡æ¿åŒ¹é…åå¤‡æ–¹æ¡ˆï¼šå½“ç‰¹å¾åŒ¹é…å¤±è´¥æ—¶ä½¿ç”¨
        
        Args:
            ref_img: å‚è€ƒå›¾åƒ
            target_img: ç›®æ ‡å›¾åƒ
            
        Returns:
            homography: ä¼°è®¡çš„å•åº”æ€§çŸ©é˜µ
        """
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(ref_img.shape) == 3:
            ref_gray = cv2.cvtColor(ref_img, cv2.COLOR_BGR2GRAY)
        else:
            ref_gray = ref_img
            
        if len(target_img.shape) == 3:
            target_gray = cv2.cvtColor(target_img, cv2.COLOR_BGR2GRAY)
        else:
            target_gray = target_img
        
        # ä½¿ç”¨å¤šå°ºåº¦æ¨¡æ¿åŒ¹é…
        best_corr = -1
        best_translation = (0, 0)
        
        scales = [0.8, 0.9, 1.0, 1.1, 1.2]
        
        for scale in scales:
            # ç¼©æ”¾ç›®æ ‡å›¾åƒ
            h, w = target_gray.shape
            new_h, new_w = int(h * scale), int(w * scale)
            scaled_target = cv2.resize(target_gray, (new_w, new_h))
            
            if scaled_target.shape[0] < ref_gray.shape[0] or scaled_target.shape[1] < ref_gray.shape[1]:
                continue
                
            # æ¨¡æ¿åŒ¹é…
            result = cv2.matchTemplate(scaled_target, ref_gray, cv2.TM_CCOEFF_NORMED)
            _, max_val, _, max_loc = cv2.minMaxLoc(result)
            
            if max_val > best_corr:
                best_corr = max_val
                # è®¡ç®—å¹³ç§»é‡ï¼ˆè€ƒè™‘ç¼©æ”¾ï¼‰
                best_translation = (
                    (max_loc[0] - (new_w - ref_gray.shape[1]) // 2) / scale,
                    (max_loc[1] - (new_h - ref_gray.shape[0]) // 2) / scale
                )
        
        # æ„å»ºå¹³ç§»çŸ©é˜µ
        if best_corr > 0.3:  # ç›¸å…³æ€§é˜ˆå€¼
            homography = np.array([
                [1, 0, best_translation[0]],
                [0, 1, best_translation[1]],
                [0, 0, 1]
            ], dtype=np.float32)
            
            logger.info(f"æ¨¡æ¿åŒ¹é…åå¤‡æ–¹æ¡ˆï¼šç›¸å…³æ€§ {best_corr:.3f}, å¹³ç§» {best_translation}")
            return homography
        
        return None
    
    def align_image(self, img, homography, reference_shape):
        """
        ä½¿ç”¨å•åº”æ€§çŸ©é˜µå¯¹é½å›¾åƒ
        
        Args:
            img: å¾…å¯¹é½çš„å›¾åƒ
            homography: å•åº”æ€§çŸ©é˜µ
            reference_shape: å‚è€ƒå›¾åƒçš„å½¢çŠ¶
            
        Returns:
            aligned_img: å¯¹é½åçš„å›¾åƒ
        """
        if homography is None:
            logger.warning("å•åº”æ€§çŸ©é˜µä¸ºç©ºï¼Œè¿”å›è°ƒæ•´å¤§å°åçš„åŸå›¾åƒ")
            return cv2.resize(img, (reference_shape[1], reference_shape[0]))
        
        # åº”ç”¨é€è§†å˜æ¢
        aligned_img = cv2.warpPerspective(
            img, homography, 
            (reference_shape[1], reference_shape[0]),
            flags=cv2.INTER_LINEAR,
            borderMode=cv2.BORDER_CONSTANT,
            borderValue=(0, 0, 0)
        )
        
        return aligned_img
    
    def process_images(self):
        """
        å¤„ç†æ‰€æœ‰å›¾åƒè¿›è¡Œå¯¹é½
        """
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = self.get_image_files()
        
        if not image_files:
            logger.error(f"åœ¨ {self.input_dir} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            return False
        
        logger.info(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
        
        # è¯»å–å‚è€ƒå›¾åƒ
        if self.reference_index >= len(image_files):
            logger.error(f"å‚è€ƒå›¾åƒç´¢å¼• {self.reference_index} è¶…å‡ºèŒƒå›´")
            return False
            
        reference_path = image_files[self.reference_index]
        reference_img = cv2.imread(reference_path)
        
        if reference_img is None:
            logger.error(f"æ— æ³•è¯»å–å‚è€ƒå›¾åƒ: {reference_path}")
            return False
        
        logger.info(f"ä½¿ç”¨å‚è€ƒå›¾åƒ: {Path(reference_path).name}")
        
        # åˆ¤æ–­å‚è€ƒå›¾åƒæ˜¯å¦ä¸ºå¤œé—´å›¾åƒ
        ref_is_night = self.is_night_image(reference_img)
        logger.info(f"å‚è€ƒå›¾åƒç±»å‹: {'å¤œé—´' if ref_is_night else 'ç™½å¤©'}")
        
        # æ£€æµ‹å‚è€ƒå›¾åƒç‰¹å¾ - å§‹ç»ˆä½¿ç”¨åŸå§‹SIFTä»¥ä¿è¯å…¼å®¹æ€§
        ref_kp, ref_desc = self.detect_features_original_sift(reference_img)
        ref_detector = "SIFT"
        
        if ref_desc is None:
            logger.error("å‚è€ƒå›¾åƒç‰¹å¾æ£€æµ‹å¤±è´¥")
            return
        
        logger.info(f"å‚è€ƒå›¾åƒä½¿ç”¨åŸå§‹SIFTæ£€æµ‹åˆ° {len(ref_kp)} ä¸ªç‰¹å¾ç‚¹")
        
        # ä¿å­˜å‚è€ƒå›¾åƒåˆ°è¾“å‡ºç›®å½•
        ref_output_path = self.output_dir / Path(reference_path).name
        cv2.imwrite(str(ref_output_path), reference_img)
        logger.info(f"ä¿å­˜å‚è€ƒå›¾åƒ: {ref_output_path}")
        
        # ç»Ÿè®¡å¤„ç†ç»“æœ
        day_count = 0
        night_count = 0
        success_count = 0
        fallback_count = 0
        copy_count = 0
        
        # å¤„ç†æŠ¥å‘Šæ•°æ®
        processing_report = []
        
        # å¤„ç†å…¶ä»–å›¾åƒ
        for i, img_path in enumerate(image_files):
            if i == self.reference_index:
                continue  # è·³è¿‡å‚è€ƒå›¾åƒ
                
            logger.info(f"å¤„ç†å›¾åƒ {i+1}/{len(image_files)}: {Path(img_path).name}")
            start_time = time.time()
            
            # è¯»å–å½“å‰å›¾åƒ
            current_img = cv2.imread(img_path)
            if current_img is None:
                logger.warning(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")
                continue
            
            # åˆ¤æ–­å›¾åƒç±»å‹
            curr_is_night = self.is_night_image(current_img)
            if curr_is_night:
                night_count += 1
            else:
                day_count += 1
            
            homography = None
            processing_method = ""
            match_points = 0
            inliers = 0
            
            if curr_is_night:
                # å¤œé—´å›¾åƒï¼šä½¿ç”¨å¢å¼ºç®—æ³•
                curr_kp, curr_desc, curr_detector = self.detect_features_adaptive(current_img, curr_is_night)
                
                if curr_desc is not None:
                    # åŒ¹é…ç‰¹å¾ç‚¹
                    matches = self.match_features_robust(ref_desc, curr_desc, ref_detector, curr_detector)
                    match_points = len(matches)
                    logger.info(f"å¤œé—´å¢å¼ºç®—æ³•æ‰¾åˆ° {match_points} ä¸ªåŒ¹é…ç‚¹")
                    
                    # ä¼°è®¡å•åº”æ€§çŸ©é˜µ
                    homography, inliers = self.estimate_homography_robust(ref_kp, curr_kp, matches)
                    
                    if homography is not None:
                        processing_method = "å¤œé—´å¢å¼ºç‰¹å¾åŒ¹é…"
                        logger.info(f"å¤œé—´å¢å¼ºç®—æ³•æˆåŠŸï¼Œå†…ç‚¹æ•°é‡: {inliers}")
                    else:
                        logger.warning("å¤œé—´å¢å¼ºç®—æ³•å¤±è´¥ï¼Œå°è¯•æ¨¡æ¿åŒ¹é…")
                
                # å¦‚æœå¢å¼ºç®—æ³•å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ¿åŒ¹é…
                if homography is None:
                    homography = self.template_matching_fallback(reference_img, current_img)
                    if homography is not None:
                        processing_method = "å¤œé—´æ¨¡æ¿åŒ¹é…"
                        fallback_count += 1
                    else:
                        processing_method = "å¤œé—´æ— å¤„ç†(ç›´æ¥å¤åˆ¶)"
                        copy_count += 1
            else:
                # ç™½å¤©å›¾åƒï¼šä½¿ç”¨åŸå§‹SIFTç®—æ³•
                curr_kp, curr_desc = self.detect_features_original_sift(current_img)
                
                if curr_desc is not None:
                    # åŒ¹é…ç‰¹å¾ç‚¹
                    matches = self.match_features_original(ref_desc, curr_desc)
                    match_points = len(matches)
                    logger.info(f"ç™½å¤©åŸå§‹SIFTç®—æ³•æ‰¾åˆ° {match_points} ä¸ªåŒ¹é…ç‚¹")
                    
                    # ä¼°è®¡å•åº”æ€§çŸ©é˜µ
                    homography = self.estimate_homography_original(ref_kp, curr_kp, matches)
                    
                    if homography is not None:
                        processing_method = "ç™½å¤©åŸå§‹SIFTåŒ¹é…"
                        logger.info(f"ç™½å¤©åŸå§‹SIFTç®—æ³•æˆåŠŸï¼ŒåŒ¹é…ç‚¹: {match_points}")
                    else:
                        logger.warning("ç™½å¤©åŸå§‹SIFTç®—æ³•å¤±è´¥ï¼Œå°è¯•æ¨¡æ¿åŒ¹é…")
                
                # å¦‚æœåŸå§‹ç®—æ³•å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ¿åŒ¹é…
                if homography is None:
                    homography = self.template_matching_fallback(reference_img, current_img)
                    if homography is not None:
                        processing_method = "ç™½å¤©æ¨¡æ¿åŒ¹é…"
                        fallback_count += 1
                    else:
                        processing_method = "ç™½å¤©æ— å¤„ç†(ç›´æ¥å¤åˆ¶)"
                        copy_count += 1
            
            # å¯¹é½å›¾åƒ
            aligned_img = self.align_image(current_img, homography, reference_img.shape)
            
            # ä¿å­˜å¯¹é½åçš„å›¾åƒ
            output_path = self.output_dir / Path(img_path).name
            cv2.imwrite(str(output_path), aligned_img)
            
            processing_time = time.time() - start_time
            image_type = "å¤œé—´" if curr_is_night else "ç™½å¤©"
            
            # è®°å½•å¤„ç†æŠ¥å‘Š
            report_entry = {
                'filename': Path(img_path).name,
                'image_type': image_type,
                'processing_method': processing_method,
                'match_points': match_points,
                'inliers': inliers,
                'processing_time': processing_time,
                'success': homography is not None
            }
            processing_report.append(report_entry)
            
            logger.info(f"ä¿å­˜å¯¹é½å›¾åƒ: {output_path} ({image_type}, {processing_method}, {processing_time:.2f}ç§’)")
            
            if homography is not None:
                success_count += 1
        
        # è¾“å‡ºå¤„ç†ç»Ÿè®¡
        total_processed = len(image_files) - 1  # æ’é™¤å‚è€ƒå›¾åƒ
        logger.info("=" * 60)
        logger.info("å¤„ç†ç»Ÿè®¡:")
        logger.info(f"æ€»å›¾åƒæ•°é‡: {total_processed}")
        logger.info(f"ç™½å¤©å›¾åƒ: {day_count}")
        logger.info(f"å¤œé—´å›¾åƒ: {night_count}")
        logger.info(f"æˆåŠŸå¯¹é½: {success_count}")
        logger.info(f"ä½¿ç”¨æ¨¡æ¿åŒ¹é…: {fallback_count}")
        logger.info(f"ç›´æ¥å¤åˆ¶: {copy_count}")
        logger.info(f"æˆåŠŸç‡: {success_count/total_processed*100:.1f}%")
        
        # ç”Ÿæˆè¯¦ç»†å¤„ç†æŠ¥å‘Š
        self.generate_processing_report(processing_report, day_count, night_count, success_count, fallback_count, copy_count)
        
        logger.info("å›¾åƒå¯¹é½å¤„ç†å®Œæˆï¼")
        return True  # è¿”å›æˆåŠŸçŠ¶æ€
    
    def generate_processing_report(self, processing_report, day_count, night_count, success_count, fallback_count, copy_count):
        """
        ç”Ÿæˆè¯¦ç»†çš„å¤„ç†æŠ¥å‘Š
        
        Args:
            processing_report: å¤„ç†æ•°æ®åˆ—è¡¨
            day_count: ç™½å¤©å›¾åƒæ•°é‡
            night_count: å¤œé—´å›¾åƒæ•°é‡
            success_count: æˆåŠŸå¤„ç†æ•°é‡
            fallback_count: ä½¿ç”¨åå¤‡æ–¹æ¡ˆæ•°é‡
            copy_count: ç›´æ¥å¤åˆ¶æ•°é‡
        """
        report_path = self.output_dir / "processing_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# å›¾åƒå¯¹é½å¤„ç†æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»å›¾åƒæ•°é‡**: {len(processing_report)}\n")
            f.write(f"- **ç™½å¤©å›¾åƒ**: {day_count} ({day_count/len(processing_report)*100:.1f}%)\n")
            f.write(f"- **å¤œé—´å›¾åƒ**: {night_count} ({night_count/len(processing_report)*100:.1f}%)\n")
            f.write(f"- **æˆåŠŸå¯¹é½**: {success_count} ({success_count/len(processing_report)*100:.1f}%)\n")
            f.write(f"- **æ¨¡æ¿åŒ¹é…**: {fallback_count} ({fallback_count/len(processing_report)*100:.1f}%)\n")
            f.write(f"- **ç›´æ¥å¤åˆ¶**: {copy_count} ({copy_count/len(processing_report)*100:.1f}%)\n\n")
            
            # ç®—æ³•æ•ˆæœç»Ÿè®¡
            f.write("## ğŸ” ç®—æ³•æ•ˆæœç»Ÿè®¡\n\n")
            
            # ç»Ÿè®¡å„ç§å¤„ç†æ–¹æ³•çš„æ•°é‡
            method_stats = {}
            for entry in processing_report:
                method = entry['processing_method']
                if method not in method_stats:
                    method_stats[method] = {'count': 0, 'success': 0, 'total_time': 0}
                method_stats[method]['count'] += 1
                if entry['success']:
                    method_stats[method]['success'] += 1
                method_stats[method]['total_time'] += entry['processing_time']
            
            f.write("| å¤„ç†æ–¹æ³• | ä½¿ç”¨æ¬¡æ•° | æˆåŠŸç‡ | å¹³å‡è€—æ—¶ |\n")
            f.write("|---------|---------|--------|----------|\n")
            for method, stats in method_stats.items():
                success_rate = stats['success'] / stats['count'] * 100
                avg_time = stats['total_time'] / stats['count']
                f.write(f"| {method} | {stats['count']} | {success_rate:.1f}% | {avg_time:.2f}s |\n")
            
            f.write("\n## ğŸ“‹ è¯¦ç»†å¤„ç†è®°å½•\n\n")
            f.write("| æ–‡ä»¶å | å›¾åƒç±»å‹ | å¤„ç†æ–¹æ³• | åŒ¹é…ç‚¹æ•° | å†…ç‚¹æ•° | è€—æ—¶(ç§’) | çŠ¶æ€ |\n")
            f.write("|--------|---------|----------|----------|--------|----------|------|\n")
            
            for entry in processing_report:
                status = "âœ… æˆåŠŸ" if entry['success'] else "âŒ å¤±è´¥"
                f.write(f"| {entry['filename']} | {entry['image_type']} | {entry['processing_method']} | ")
                f.write(f"{entry['match_points']} | {entry['inliers']} | {entry['processing_time']:.2f} | {status} |\n")
            
            # åˆ†ç±»ç»Ÿè®¡
            f.write("\n## ğŸ“ˆ åˆ†ç±»è¯¦ç»†ç»Ÿè®¡\n\n")
            
            # ç™½å¤©å›¾åƒç»Ÿè®¡
            day_images = [e for e in processing_report if e['image_type'] == 'ç™½å¤©']
            if day_images:
                day_success = len([e for e in day_images if e['success']])
                f.write(f"### ç™½å¤©å›¾åƒ ({len(day_images)}å¼ )\n")
                f.write(f"- æˆåŠŸç‡: {day_success/len(day_images)*100:.1f}%\n")
                f.write(f"- åŸå§‹SIFTæˆåŠŸ: {len([e for e in day_images if 'åŸå§‹SIFT' in e['processing_method'] and e['success']])}å¼ \n")
                f.write(f"- æ¨¡æ¿åŒ¹é…: {len([e for e in day_images if 'æ¨¡æ¿åŒ¹é…' in e['processing_method']])}å¼ \n")
                f.write(f"- ç›´æ¥å¤åˆ¶: {len([e for e in day_images if 'ç›´æ¥å¤åˆ¶' in e['processing_method']])}å¼ \n\n")
            
            # å¤œé—´å›¾åƒç»Ÿè®¡
            night_images = [e for e in processing_report if e['image_type'] == 'å¤œé—´']
            if night_images:
                night_success = len([e for e in night_images if e['success']])
                f.write(f"### å¤œé—´å›¾åƒ ({len(night_images)}å¼ )\n")
                f.write(f"- æˆåŠŸç‡: {night_success/len(night_images)*100:.1f}%\n")
                f.write(f"- å¢å¼ºç®—æ³•æˆåŠŸ: {len([e for e in night_images if 'å¢å¼ºç‰¹å¾' in e['processing_method'] and e['success']])}å¼ \n")
                f.write(f"- æ¨¡æ¿åŒ¹é…: {len([e for e in night_images if 'æ¨¡æ¿åŒ¹é…' in e['processing_method']])}å¼ \n")
                f.write(f"- ç›´æ¥å¤åˆ¶: {len([e for e in night_images if 'ç›´æ¥å¤åˆ¶' in e['processing_method']])}å¼ \n\n")
            
            f.write("\n---\n")
            f.write("*æŠ¥å‘Šç”Ÿæˆäº Enhanced TickTock-Align-NPU Library*\n")
        
        logger.info(f"è¯¦ç»†å¤„ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
